{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1a-gG-poXnVnOy4qXTRoLBl17t6uEbHqb",
      "authorship_tag": "ABX9TyMx+L9ffHcII4H6bHLyVPvq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ayachiii/Flow_study/blob/main/joint_analysis_pli_lf.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "driveå†…ã®ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ã€pliã¨lfã®10äººåˆ†5ãƒ•ã‚§ãƒ¼ã‚ºã®ãƒ‡ãƒ¼ã‚¿ã‚’æç”»ã—ã€ãã‚Œã‚’ãã£ã¤ã‘ã¦ä¸€ã¤ã®ç”»åƒã‚’ä½œæˆã—ãŸã€‚"
      ],
      "metadata": {
        "id": "O81BuhKtCCA2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        
        "# --- 1. ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã¨ã‚¤ãƒ³ãƒãƒ¼ãƒˆ ---\n",
        "import sys\n",
        "import os\n",
        "import glob\n",
        "import shutil\n",
        "import warnings\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from google.colab import drive\n",
        "\n",
        "# HeartPyãŒãªã„å ´åˆã¯ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«\n",
        "try:\n",
        "    import heartpy as hp\n",
        "except ImportError:\n",
        "    !pip install heartpy\n",
        "    import heartpy as hp\n",
        "\n",
        "# è­¦å‘Šéè¡¨ç¤º\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# --- 2. è¨­å®šã‚¨ãƒªã‚¢ (ã“ã“ã ã‘ç’°å¢ƒã«åˆã‚ã›ã¦å¤‰æ›´ã—ã¦ãã ã•ã„) ---\n",
        "\n",
        "# Google Driveã‚’ãƒã‚¦ãƒ³ãƒˆ\n",
        "if not os.path.exists('/content/drive'):\n",
        "    drive.mount('/content/drive')\n",
        "\n",
        "# â˜…é‡è¦: ãƒ‡ãƒ¼ã‚¿ãŒæ ¼ç´ã•ã‚Œã¦ã„ã‚‹è¦ªãƒ•ã‚©ãƒ«ãƒ€ã®ãƒ‘ã‚¹\n",
        "# ã“ã®ãƒ•ã‚©ãƒ«ãƒ€ã®ä¸‹ã«ã‚ã‚‹ã™ã¹ã¦ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¤œç´¢ã—ã«è¡Œãã¾ã™\n",
        "base_search_path = \"/content/drive/MyDrive/csv\"\n",
        "\n",
        "# è¢«é¨“è€…ãƒªã‚¹ãƒˆ (ãƒ•ã‚©ãƒ«ãƒ€åã‚„ãƒ•ã‚¡ã‚¤ãƒ«åã«å«ã¾ã‚Œã‚‹åå‰)\n",
        "subject_list = [\n",
        "    'ishida', 'yamamoto', 'ohashi', 'miyake','mitsuhashi',\n",
        "    'ko', 'hayato','chika', 'nagomi', 'mio'\n",
        "]\n",
        "\n",
        "# ãƒ•ã‚§ãƒ¼ã‚ºãƒªã‚¹ãƒˆ\n",
        "phase_list = ['rest','boredom', 'flow', 'flow_ultra', 'overload']\n",
        "\n",
        "# ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°å‘¨æ³¢æ•° (ECGè¨ˆæ¸¬æ™‚ã®Hzã€‚ä¸€èˆ¬çš„ã«256, 500, 1000ãªã©)\n",
        "FS = 256\n",
        "\n",
        "# çµæœä¿å­˜ãƒ•ã‚©ãƒ«ãƒ€\n",
        "output_dir = \"All_Participants_Graphs\"\n",
        "if os.path.exists(output_dir):\n",
        "    shutil.rmtree(output_dir) # å¤ã„ãƒ•ã‚©ãƒ«ãƒ€ãŒã‚ã‚Œã°æ¶ˆã™\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "\n",
        "# --- 3. ä¾¿åˆ©ãªé–¢æ•°ç¾¤ ---\n",
        "\n",
        "def find_file_smart(base_path, subject, phase, keywords):\n",
        "    \"\"\"\n",
        "    æŒ‡å®šã—ãŸã€Œè¢«é¨“è€…ã€ã€Œãƒ•ã‚§ãƒ¼ã‚ºã€ã€Œã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰(ECGã‚„PLIãªã©)ã€ã‚’å«ã‚€ãƒ•ã‚¡ã‚¤ãƒ«ã‚’\n",
        "    ãƒ•ã‚©ãƒ«ãƒ€éšå±¤ã®å¥¥æ·±ãã¾ã§è‡ªå‹•æ¤œç´¢ã™ã‚‹é–¢æ•°\n",
        "    \"\"\"\n",
        "    # æ¤œç´¢ãƒ‘ã‚¿ãƒ¼ãƒ³: base_pathä»¥ä¸‹ã®ã™ã¹ã¦ã®éšå±¤(**)ã‹ã‚‰ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¢ã™\n",
        "    # ä¾‹: *ishida*rest*ECG*.csv\n",
        "    search_pattern = os.path.join(base_path, \"**\", f\"*{subject}*{phase}*.csv\")\n",
        "    candidates = glob.glob(search_pattern, recursive=True)\n",
        "\n",
        "    for path in candidates:\n",
        "        filename = os.path.basename(path)\n",
        "        # æŒ‡å®šã—ãŸã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰(ECG, PLIãªã©)ãŒå«ã¾ã‚Œã¦ã„ã‚‹ã‹ãƒã‚§ãƒƒã‚¯\n",
        "        if all(k in filename for k in keywords):\n",
        "            return path\n",
        "    return None\n",
        "\n",
        "def get_column_data(df, target_names):\n",
        "    \"\"\"\n",
        "    ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã‹ã‚‰ã€ã‚«ãƒ©ãƒ åãŒå¤§æ–‡å­—å°æ–‡å­—é•ã£ã¦ã‚‚æŸ”è»Ÿã«ãƒ‡ãƒ¼ã‚¿ã‚’å–ã‚Šå‡ºã™é–¢æ•°\n",
        "    ä¾‹: 'FCz', 'fcz', ' FCz ' ãªã©ã‚’åŒä¸€è¦–ã—ã¦å–å¾—\n",
        "    \"\"\"\n",
        "    df.columns = [c.strip() for c in df.columns] # ç©ºç™½å‰Šé™¤\n",
        "    for col in df.columns:\n",
        "        for target in target_names:\n",
        "            if col.lower() == target.lower():\n",
        "                return df[col], col # ãƒ‡ãƒ¼ã‚¿ã¨è¦‹ã¤ã‹ã£ãŸæ­£ç¢ºãªåˆ—åã‚’è¿”ã™\n",
        "    return None, None\n",
        "\n",
        "def calculate_lf_timeseries(ecg_data, fs):\n",
        "    \"\"\"\n",
        "    ECGç”Ÿãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ HeartPy ã‚’ä½¿ã£ã¦ LF(Low Frequency) ã®æ™‚ç³»åˆ—ãƒ‡ãƒ¼ã‚¿ã‚’è¨ˆç®—ã™ã‚‹\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # segment_width: è¨ˆç®—ã™ã‚‹çª“ã®å¹…(ç§’)ã€‚LFã‚’å‡ºã™ã«ã¯30ç§’ä»¥ä¸Šæ¨å¥¨ã€‚\n",
        "        # segment_overlap: çª“ã‚’ãšã‚‰ã™å‰²åˆ(0.5ãªã‚‰50%é‡è¤‡)\n",
        "        wd, m = hp.process_segmentwise(ecg_data.values, sample_rate=fs, segment_width=40, segment_overlap=0.5)\n",
        "\n",
        "        if 'lf' in m:\n",
        "            # è¨ˆç®—ã•ã‚ŒãŸLFå€¤ (ãƒ‡ãƒ¼ã‚¿ç‚¹æ•°ã¯å°‘ãªã„)\n",
        "            lf_values = m['lf']\n",
        "\n",
        "            # å…ƒã®ãƒ‡ãƒ¼ã‚¿ã®é•·ã•ã«åˆã‚ã›ã¦å¼•ãä¼¸ã°ã™ï¼ˆç·šå½¢è£œé–“ï¼‰\n",
        "            x_original = np.linspace(0, len(ecg_data), len(ecg_data))\n",
        "            x_calculated = np.linspace(0, len(ecg_data), len(lf_values))\n",
        "            lf_interpolated = np.interp(x_original, x_calculated, lf_values)\n",
        "\n",
        "            return pd.Series(lf_interpolated, index=ecg_data.index)\n",
        "        else:\n",
        "            return None\n",
        "    except Exception:\n",
        "        return None\n",
        "\n",
        "def min_max_norm(series):\n",
        "    \"\"\"ã‚°ãƒ©ãƒ•é‡ã­åˆã‚ã›ç”¨ã«0-1ã«æ­£è¦åŒ–\"\"\"\n",
        "    if len(series) == 0: return series\n",
        "    scaler = MinMaxScaler()\n",
        "    return scaler.fit_transform(series.values.reshape(-1, 1)).flatten()\n",
        "\n",
        "\n",
        "# --- 4. ãƒ¡ã‚¤ãƒ³å‡¦ç†ãƒ«ãƒ¼ãƒ— ---\n",
        "\n",
        "print(f\"ğŸš€ è§£æé–‹å§‹: å…¨ {len(subject_list)*len(phase_list)} ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’å‡¦ç†ã—ã¾ã™...\\n\")\n",
        "\n",
        "success_count = 0\n",
        "error_count = 0\n",
        "\n",
        "for subject in subject_list:\n",
        "    for phase in phase_list:\n",
        "        print(f\"[{subject} - {phase}] Processing...\", end=\" \")\n",
        "\n",
        "        # --- A. ãƒ•ã‚¡ã‚¤ãƒ«æ¤œç´¢ ---\n",
        "        # è„³æ³¢ãƒ‡ãƒ¼ã‚¿ (PLIãªã©) ã‚’æ¢ã™\n",
        "        path_eeg = find_file_smart(base_search_path, subject, phase, keywords=[\"PLI\"])\n",
        "        # å¿ƒæ‹ãƒ‡ãƒ¼ã‚¿ (ECG) ã‚’æ¢ã™\n",
        "        path_ecg = find_file_smart(base_search_path, subject, phase, keywords=[\"ECG\"])\n",
        "\n",
        "        if not path_eeg or not path_ecg:\n",
        "            print(f\"âŒ ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ (EEG: {'OK' if path_eeg else 'Missing'}, ECG: {'OK' if path_ecg else 'Missing'})\")\n",
        "            error_count += 1\n",
        "            continue\n",
        "\n",
        "        try:\n",
        "            # --- B. ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ & æŠ½å‡º ---\n",
        "            df_eeg = pd.read_csv(path_eeg)\n",
        "            df_ecg = pd.read_csv(path_ecg)\n",
        "\n",
        "            # FCz (è„³æ³¢) ã®æŠ½å‡º\n",
        "            eeg_data, eeg_col_name = get_column_data(df_eeg, ['FCz', 'Fz', 'Cz']) # å€™è£œãƒªã‚¹ãƒˆ\n",
        "\n",
        "            # LF (å¿ƒæ‹) ã®æº–å‚™\n",
        "            # 1. æ—¢ã«è¨ˆç®—æ¸ˆã¿ã® 'lf' åˆ—ãŒã‚ã‚‹ã‹æ¢ã™\n",
        "            lf_data, lf_col_name = get_column_data(df_ecg, ['lf', 'LF'])\n",
        "\n",
        "            # 2. ãªã‘ã‚Œã° 'ECG' åˆ—ã‹ã‚‰è¨ˆç®—ã™ã‚‹\n",
        "            if lf_data is None:\n",
        "                ecg_raw, ecg_col_name = get_column_data(df_ecg, ['ECG', 'ecg'])\n",
        "                if ecg_raw is not None:\n",
        "                    # print(\"(Calculating LF...)\", end=\" \")\n",
        "                    lf_data = calculate_lf_timeseries(ecg_raw, FS)\n",
        "                    lf_col_name = \"Calculated LF\"\n",
        "\n",
        "            # ä¸¡æ–¹ã®ãƒ‡ãƒ¼ã‚¿ãŒæƒã£ãŸã‹ç¢ºèª\n",
        "            if eeg_data is None or lf_data is None:\n",
        "                print(f\"âŒ åˆ—ãƒ‡ãƒ¼ã‚¿ä¸è¶³ (FCz: {'OK' if eeg_data is not None else 'Missing'}, LF: {'OK' if lf_data is not None else 'Missing'})\")\n",
        "                error_count += 1\n",
        "                continue\n",
        "\n",
        "            # --- C. ãƒ‡ãƒ¼ã‚¿æ•´å½¢ & æç”» ---\n",
        "\n",
        "            # é•·ã•ã‚’æƒãˆã‚‹ (çŸ­ã„æ–¹ã«åˆã‚ã›ã‚‹)\n",
        "            min_len = min(len(eeg_data), len(lf_data))\n",
        "            eeg_plot = eeg_data.iloc[:min_len]\n",
        "            lf_plot = lf_data.iloc[:min_len]\n",
        "\n",
        "            # ç§»å‹•å¹³å‡ã§ã‚¹ãƒ ãƒ¼ã‚¸ãƒ³ã‚° (ã‚°ãƒ©ãƒ•ã‚’è¦‹ã‚„ã™ãã™ã‚‹ãŸã‚)\n",
        "            window = 50\n",
        "            eeg_smooth = eeg_plot.rolling(window=window).mean()\n",
        "            lf_smooth = lf_plot.rolling(window=window).mean()\n",
        "\n",
        "            # ãƒ—ãƒ­ãƒƒãƒˆä½œæˆ\n",
        "            fig, ax1 = plt.subplots(figsize=(12, 5))\n",
        "\n",
        "            # å·¦è»¸: EEG (é’)\n",
        "            ax1.plot(min_max_norm(eeg_smooth), color='blue', label=f'EEG ({eeg_col_name})', alpha=0.7)\n",
        "            ax1.set_ylabel('Normalized EEG Power', color='blue')\n",
        "            ax1.tick_params(axis='y', labelcolor='blue')\n",
        "            ax1.set_xlabel('Time (samples)')\n",
        "\n",
        "            # å³è»¸: HRV (ã‚ªãƒ¬ãƒ³ã‚¸)\n",
        "            ax2 = ax1.twinx()\n",
        "            ax2.plot(min_max_norm(lf_smooth), color='orange', label=f'HRV ({lf_col_name})', alpha=0.7)\n",
        "            ax2.set_ylabel('Normalized LF Power', color='orange')\n",
        "            ax2.tick_params(axis='y', labelcolor='orange')\n",
        "\n",
        "            # ã‚¿ã‚¤ãƒˆãƒ«ã¨ãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆ\n",
        "            plt.title(f\"Subject: {subject} | Phase: {phase}\")\n",
        "\n",
        "            # å‡¡ä¾‹ã‚’ã¾ã¨ã‚ã¦è¡¨ç¤º\n",
        "            lines1, labels1 = ax1.get_legend_handles_labels()\n",
        "            lines2, labels2 = ax2.get_legend_handles_labels()\n",
        "            ax1.legend(lines1 + lines2, labels1 + labels2, loc='upper left')\n",
        "\n",
        "            # ä¿å­˜\n",
        "            save_name = f\"{subject}_{phase}_FCz_LF.png\"\n",
        "            plt.savefig(os.path.join(output_dir, save_name))\n",
        "            plt.close() # ãƒ¡ãƒ¢ãƒªè§£æ”¾\n",
        "\n",
        "            print(\"âœ… å®Œäº†ãƒ»ä¿å­˜\")\n",
        "            success_count += 1\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"âŒ ã‚¨ãƒ©ãƒ¼ç™ºç”Ÿ: {e}\")\n",
        "            error_count += 1\n",
        "\n",
        "# --- 5. çµæœã®åœ§ç¸®ã¨ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ ---\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(f\"å…¨ã¦ã®å‡¦ç†ãŒçµ‚äº†ã—ã¾ã—ãŸ (æˆåŠŸ: {success_count}, å¤±æ•—: {error_count})\")\n",
        "\n",
        "if success_count > 0:\n",
        "    print(\"ğŸ“¦ ç”»åƒã‚’Zipã«ã¾ã¨ã‚ã¦ã„ã¾ã™...\")\n",
        "    shutil.make_archive('Results_All_Participants', 'zip', output_dir)\n",
        "    print(\"â¬‡ï¸ ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã‚’é–‹å§‹ã—ã¾ã™\")\n",
        "    from google.colab import files\n",
        "    files.download('Results_All_Participants.zip')\n",
        "else:\n",
        "    print(\"âš ï¸ ç”»åƒãŒ1æšã‚‚ä½œæˆã•ã‚Œã¾ã›ã‚“ã§ã—ãŸã€‚ãƒ‘ã‚¹è¨­å®šã‚„ãƒ‡ãƒ¼ã‚¿ã®ä¸­èº«ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xnJzwP2zSkyQ",
        "outputId": "441eecba-199a-4b3c-eb93-fceca13219a4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸš€ è§£æé–‹å§‹: å…¨ 50 ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’å‡¦ç†ã—ã¾ã™...\n",
            "\n",
            "[ishida - rest] Processing... âŒ åˆ—ãƒ‡ãƒ¼ã‚¿ä¸è¶³ (FCz: OK, LF: Missing)\n",
            "[ishida - boredom] Processing... âŒ åˆ—ãƒ‡ãƒ¼ã‚¿ä¸è¶³ (FCz: OK, LF: Missing)\n",
            "[ishida - flow] Processing... âŒ åˆ—ãƒ‡ãƒ¼ã‚¿ä¸è¶³ (FCz: OK, LF: Missing)\n",
            "[ishida - flow_ultra] Processing... âŒ åˆ—ãƒ‡ãƒ¼ã‚¿ä¸è¶³ (FCz: OK, LF: Missing)\n",
            "[ishida - overload] Processing... âŒ åˆ—ãƒ‡ãƒ¼ã‚¿ä¸è¶³ (FCz: OK, LF: Missing)\n",
            "[yamamoto - rest] Processing... âŒ åˆ—ãƒ‡ãƒ¼ã‚¿ä¸è¶³ (FCz: OK, LF: Missing)\n",
            "[yamamoto - boredom] Processing... âŒ åˆ—ãƒ‡ãƒ¼ã‚¿ä¸è¶³ (FCz: OK, LF: Missing)\n",
            "[yamamoto - flow] Processing... âŒ åˆ—ãƒ‡ãƒ¼ã‚¿ä¸è¶³ (FCz: OK, LF: Missing)\n",
            "[yamamoto - flow_ultra] Processing... âŒ åˆ—ãƒ‡ãƒ¼ã‚¿ä¸è¶³ (FCz: OK, LF: Missing)\n",
            "[yamamoto - overload] Processing... âŒ åˆ—ãƒ‡ãƒ¼ã‚¿ä¸è¶³ (FCz: OK, LF: Missing)\n",
            "[ohashi - rest] Processing... âŒ åˆ—ãƒ‡ãƒ¼ã‚¿ä¸è¶³ (FCz: OK, LF: Missing)\n",
            "[ohashi - boredom] Processing... âŒ åˆ—ãƒ‡ãƒ¼ã‚¿ä¸è¶³ (FCz: OK, LF: Missing)\n",
            "[ohashi - flow] Processing... âŒ åˆ—ãƒ‡ãƒ¼ã‚¿ä¸è¶³ (FCz: OK, LF: Missing)\n",
            "[ohashi - flow_ultra] Processing... âŒ åˆ—ãƒ‡ãƒ¼ã‚¿ä¸è¶³ (FCz: OK, LF: Missing)\n",
            "[ohashi - overload] Processing... âŒ åˆ—ãƒ‡ãƒ¼ã‚¿ä¸è¶³ (FCz: OK, LF: Missing)\n",
            "[miyake - rest] Processing... âŒ åˆ—ãƒ‡ãƒ¼ã‚¿ä¸è¶³ (FCz: OK, LF: Missing)\n",
            "[miyake - boredom] Processing... âŒ åˆ—ãƒ‡ãƒ¼ã‚¿ä¸è¶³ (FCz: OK, LF: Missing)\n",
            "[miyake - flow] Processing... âŒ åˆ—ãƒ‡ãƒ¼ã‚¿ä¸è¶³ (FCz: OK, LF: Missing)\n",
            "[miyake - flow_ultra] Processing... âŒ åˆ—ãƒ‡ãƒ¼ã‚¿ä¸è¶³ (FCz: OK, LF: Missing)\n",
            "[miyake - overload] Processing... âŒ åˆ—ãƒ‡ãƒ¼ã‚¿ä¸è¶³ (FCz: OK, LF: Missing)\n",
            "[mitsuhashi - rest] Processing... âŒ åˆ—ãƒ‡ãƒ¼ã‚¿ä¸è¶³ (FCz: OK, LF: Missing)\n",
            "[mitsuhashi - boredom] Processing... âŒ åˆ—ãƒ‡ãƒ¼ã‚¿ä¸è¶³ (FCz: OK, LF: Missing)\n",
            "[mitsuhashi - flow] Processing... âŒ åˆ—ãƒ‡ãƒ¼ã‚¿ä¸è¶³ (FCz: OK, LF: Missing)\n",
            "[mitsuhashi - flow_ultra] Processing... âŒ åˆ—ãƒ‡ãƒ¼ã‚¿ä¸è¶³ (FCz: OK, LF: Missing)\n",
            "[mitsuhashi - overload] Processing... âŒ åˆ—ãƒ‡ãƒ¼ã‚¿ä¸è¶³ (FCz: OK, LF: Missing)\n",
            "[ko - rest] Processing... âŒ åˆ—ãƒ‡ãƒ¼ã‚¿ä¸è¶³ (FCz: OK, LF: Missing)\n",
            "[ko - boredom] Processing... âŒ åˆ—ãƒ‡ãƒ¼ã‚¿ä¸è¶³ (FCz: OK, LF: Missing)\n",
            "[ko - flow] Processing... âŒ åˆ—ãƒ‡ãƒ¼ã‚¿ä¸è¶³ (FCz: OK, LF: Missing)\n",
            "[ko - flow_ultra] Processing... âŒ åˆ—ãƒ‡ãƒ¼ã‚¿ä¸è¶³ (FCz: OK, LF: Missing)\n",
            "[ko - overload] Processing... âŒ åˆ—ãƒ‡ãƒ¼ã‚¿ä¸è¶³ (FCz: OK, LF: Missing)\n",
            "[hayato - rest] Processing... âŒ åˆ—ãƒ‡ãƒ¼ã‚¿ä¸è¶³ (FCz: OK, LF: Missing)\n",
            "[hayato - boredom] Processing... âŒ åˆ—ãƒ‡ãƒ¼ã‚¿ä¸è¶³ (FCz: OK, LF: Missing)\n",
            "[hayato - flow] Processing... âŒ åˆ—ãƒ‡ãƒ¼ã‚¿ä¸è¶³ (FCz: OK, LF: Missing)\n",
            "[hayato - flow_ultra] Processing... âŒ åˆ—ãƒ‡ãƒ¼ã‚¿ä¸è¶³ (FCz: OK, LF: Missing)\n",
            "[hayato - overload] Processing... âŒ åˆ—ãƒ‡ãƒ¼ã‚¿ä¸è¶³ (FCz: OK, LF: Missing)\n",
            "[chika - rest] Processing... âŒ åˆ—ãƒ‡ãƒ¼ã‚¿ä¸è¶³ (FCz: OK, LF: Missing)\n",
            "[chika - boredom] Processing... âŒ åˆ—ãƒ‡ãƒ¼ã‚¿ä¸è¶³ (FCz: OK, LF: Missing)\n",
            "[chika - flow] Processing... âŒ åˆ—ãƒ‡ãƒ¼ã‚¿ä¸è¶³ (FCz: OK, LF: Missing)\n",
            "[chika - flow_ultra] Processing... âŒ åˆ—ãƒ‡ãƒ¼ã‚¿ä¸è¶³ (FCz: OK, LF: Missing)\n",
            "[chika - overload] Processing... âŒ åˆ—ãƒ‡ãƒ¼ã‚¿ä¸è¶³ (FCz: OK, LF: Missing)\n",
            "[nagomi - rest] Processing... âŒ åˆ—ãƒ‡ãƒ¼ã‚¿ä¸è¶³ (FCz: OK, LF: Missing)\n",
            "[nagomi - boredom] Processing... âŒ åˆ—ãƒ‡ãƒ¼ã‚¿ä¸è¶³ (FCz: OK, LF: Missing)\n",
            "[nagomi - flow] Processing... âŒ åˆ—ãƒ‡ãƒ¼ã‚¿ä¸è¶³ (FCz: OK, LF: Missing)\n",
            "[nagomi - flow_ultra] Processing... âŒ åˆ—ãƒ‡ãƒ¼ã‚¿ä¸è¶³ (FCz: OK, LF: Missing)\n",
            "[nagomi - overload] Processing... âŒ åˆ—ãƒ‡ãƒ¼ã‚¿ä¸è¶³ (FCz: OK, LF: Missing)\n",
            "[mio - rest] Processing... âŒ åˆ—ãƒ‡ãƒ¼ã‚¿ä¸è¶³ (FCz: OK, LF: Missing)\n",
            "[mio - boredom] Processing... âŒ åˆ—ãƒ‡ãƒ¼ã‚¿ä¸è¶³ (FCz: OK, LF: Missing)\n",
            "[mio - flow] Processing... âŒ åˆ—ãƒ‡ãƒ¼ã‚¿ä¸è¶³ (FCz: OK, LF: Missing)\n",
            "[mio - flow_ultra] Processing... âŒ åˆ—ãƒ‡ãƒ¼ã‚¿ä¸è¶³ (FCz: OK, LF: Missing)\n",
            "[mio - overload] Processing... âŒ åˆ—ãƒ‡ãƒ¼ã‚¿ä¸è¶³ (FCz: OK, LF: Missing)\n",
            "\n",
            "==================================================\n",
            "å…¨ã¦ã®å‡¦ç†ãŒçµ‚äº†ã—ã¾ã—ãŸ (æˆåŠŸ: 0, å¤±æ•—: 50)\n",
            "âš ï¸ ç”»åƒãŒ1æšã‚‚ä½œæˆã•ã‚Œã¾ã›ã‚“ã§ã—ãŸã€‚ãƒ‘ã‚¹è¨­å®šã‚„ãƒ‡ãƒ¼ã‚¿ã®ä¸­èº«ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# ã€RRé–“éš”å¯¾å¿œç‰ˆã€‘EEG(FCz) & HRV(LF from RR) è§£æã‚³ãƒ¼ãƒ‰\n",
        "# =============================================================================\n",
        "\n",
        "import sys\n",
        "import os\n",
        "import glob\n",
        "import shutil\n",
        "import warnings\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy import signal\n",
        "from scipy.interpolate import interp1d\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from google.colab import drive\n",
        "\n",
        "# è­¦å‘Šéè¡¨ç¤º\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# --- 1. è¨­å®šã‚¨ãƒªã‚¢ ---\n",
        "\n",
        "if not os.path.exists('/content/drive'):\n",
        "    drive.mount('/content/drive')\n",
        "\n",
        "# è¦ªãƒ•ã‚©ãƒ«ãƒ€ã®ãƒ‘ã‚¹ (ã“ã“ã‹ã‚‰ä¸‹ã‚’å…¨éƒ¨æ¢ã—ã¾ã™)\n",
        "base_search_path = \"/content/drive/MyDrive/csv\"\n",
        "\n",
        "# è¢«é¨“è€…ãƒªã‚¹ãƒˆ (ãƒ•ã‚©ãƒ«ãƒ€åã‚„ãƒ•ã‚¡ã‚¤ãƒ«åã«å«ã¾ã‚Œã‚‹åå‰)\n",
        "subject_list = [\n",
        "    'ishida', 'yamamoto', 'ohashi', 'miyake','mitsuhashi',\n",
        "    'ko', 'hayato','chika', 'nagomi', 'mio'\n",
        "]\n",
        "\n",
        "# ãƒ•ã‚§ãƒ¼ã‚ºãƒªã‚¹ãƒˆ\n",
        "phase_list = ['rest','boredom', 'flow', 'flow_ultra', 'overload']\n",
        "\n",
        "# çµæœä¿å­˜ãƒ•ã‚©ãƒ«ãƒ€\n",
        "output_dir = \"Graphs_FCz_LF_from_RR\"\n",
        "if os.path.exists(output_dir):\n",
        "    shutil.rmtree(output_dir)\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "\n",
        "# --- 2. é–¢æ•°ç¾¤ ---\n",
        "\n",
        "def find_file_fuzzy(base_path, subject, phase, file_type_keywords):\n",
        "    \"\"\"\n",
        "    ãƒ•ã‚¡ã‚¤ãƒ«åã«ã€Œåå‰ã€ã€Œãƒ•ã‚§ãƒ¼ã‚ºã€ã€Œã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰(RRãªã©)ã€ã‚’å«ã‚€ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¢ã™\n",
        "    \"\"\"\n",
        "    # æ¤œç´¢: *ishida*flow*RR*.csv ã®ã‚ˆã†ãªãƒ‘ã‚¿ãƒ¼ãƒ³\n",
        "    # ãƒ•ã‚¡ã‚¤ãƒ«ã‚¿ã‚¤ãƒ—ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãŒè¤‡æ•°ã‚ã‚‹å ´åˆ (ä¾‹: ['RR', 'rr']) ã¯ã©ã‚Œã‹ä¸€ã¤ã‚ã‚Œã°OK\n",
        "    search_pattern = os.path.join(base_path, \"**\", f\"*{subject}*{phase}*.csv\")\n",
        "    candidates = glob.glob(search_pattern, recursive=True)\n",
        "\n",
        "    for path in candidates:\n",
        "        filename = os.path.basename(path)\n",
        "        # æŒ‡å®šã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã®ã„ãšã‚Œã‹ãŒå«ã¾ã‚Œã¦ã„ã‚‹ã‹\n",
        "        if any(k in filename for k in file_type_keywords):\n",
        "            return path\n",
        "    return None\n",
        "\n",
        "def get_column_data(df, target_names):\n",
        "    \"\"\"åˆ—åãŒã‚ã„ã¾ã„ã§ã‚‚ãƒ‡ãƒ¼ã‚¿ã‚’å–ã‚Šå‡ºã™\"\"\"\n",
        "    df.columns = [c.strip() for c in df.columns]\n",
        "    for col in df.columns:\n",
        "        for target in target_names:\n",
        "            if col.lower() == target.lower():\n",
        "                return df[col], col\n",
        "    return None, None\n",
        "\n",
        "def calculate_lf_from_rr(rr_data_ms):\n",
        "    \"\"\"\n",
        "    RRé–“éš”(ms)ã®ãƒªã‚¹ãƒˆã‹ã‚‰ã€LFæˆåˆ†ã®æ™‚ç³»åˆ—å¤‰åŒ–ã‚’è¨ˆç®—ã™ã‚‹\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # 1. å¤–ã‚Œå€¤é™¤å» (ç•°å¸¸ãªRRé–“éš”ã‚’ã‚«ãƒƒãƒˆ: 300ms~1300msç¨‹åº¦ãŒæ­£å¸¸ç¯„å›²)\n",
        "        rr_data = rr_data_ms[(rr_data_ms > 300) & (rr_data_ms < 1300)]\n",
        "\n",
        "        if len(rr_data) < 10: return None # ãƒ‡ãƒ¼ã‚¿å°‘ãªã™ã\n",
        "\n",
        "        # 2. æ™‚ç³»åˆ—ã¸ã®å¤‰æ› (è£œé–“)\n",
        "        # RRé–“éš”ã®ç´¯ç©å’Œã‚’ã€Œæ™‚é–“è»¸(X)ã€ã¨ã™ã‚‹\n",
        "        t_cumulative = np.cumsum(rr_data) / 1000.0 # ç§’å˜ä½ã«å¤‰æ›\n",
        "        t_cumulative = t_cumulative - t_cumulative.iloc[0] # 0ç§’ã‚¹ã‚¿ãƒ¼ãƒˆã«\n",
        "\n",
        "        # 4Hz (0.25ç§’åˆ»ã¿) ã§ç­‰é–“éš”ã«ãƒªã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°\n",
        "        fs_interp = 4.0\n",
        "        t_interp = np.arange(0, t_cumulative.iloc[-1], 1/fs_interp)\n",
        "\n",
        "        f_interp = interp1d(t_cumulative, rr_data, kind='cubic', fill_value=\"extrapolate\")\n",
        "        rr_interp = f_interp(t_interp)\n",
        "\n",
        "        # 3. LFãƒ‘ãƒ¯ãƒ¼ã®æ™‚ç³»åˆ—è¨ˆç®— (ã‚¹ãƒ©ã‚¤ãƒ‡ã‚£ãƒ³ã‚°ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦)\n",
        "        # çª“å¹…: 60ç§’, ã‚¹ãƒ†ãƒƒãƒ—: 10ç§’\n",
        "        window_sec = 60\n",
        "        step_sec = 10\n",
        "        nperseg = int(window_sec * fs_interp)\n",
        "        step_samples = int(step_sec * fs_interp)\n",
        "\n",
        "        lf_series = []\n",
        "        time_series = []\n",
        "\n",
        "        for i in range(0, len(rr_interp) - nperseg, step_samples):\n",
        "            segment = rr_interp[i : i + nperseg]\n",
        "\n",
        "            # ã‚¦ã‚§ãƒ«ãƒæ³•ã§ãƒ‘ãƒ¯ãƒ¼ã‚¹ãƒšã‚¯ãƒˆãƒ«å¯†åº¦(PSD)ã‚’è¨ˆç®—\n",
        "            freqs, psd = signal.welch(segment, fs=fs_interp, nperseg=nperseg)\n",
        "\n",
        "            # LFå¸¯åŸŸ (0.04 - 0.15 Hz) ã®ãƒ‘ãƒ¯ãƒ¼ã‚’ç©åˆ†\n",
        "            lf_band = (freqs >= 0.04) & (freqs <= 0.15)\n",
        "            lf_power = np.trapz(psd[lf_band], freqs[lf_band])\n",
        "\n",
        "            lf_series.append(lf_power)\n",
        "            # æ™‚åˆ»ã¯çª“ã®ä¸­å¿ƒ\n",
        "            current_time = t_interp[i + nperseg//2]\n",
        "            time_series.append(current_time)\n",
        "\n",
        "        # å…ƒã®ãƒ‡ãƒ¼ã‚¿ã®é•·ã•ã«åˆã‚ã›ã¦å¼•ãä¼¸ã°ã™ï¼ˆã‚°ãƒ©ãƒ•æç”»ç”¨ï¼‰\n",
        "        # å…ƒã®index (ãƒ‡ãƒ¼ã‚¿ç‚¹æ•°) ã«åˆã‚ã›ã‚‹ãŸã‚ã®è£œé–“\n",
        "        x_target = np.linspace(0, t_cumulative.iloc[-1], len(rr_data_ms))\n",
        "        if len(time_series) > 1:\n",
        "            f_resample = interp1d(time_series, lf_series, kind='linear', fill_value=\"extrapolate\")\n",
        "            lf_final = f_resample(x_target)\n",
        "            return pd.Series(lf_final, index=rr_data_ms.index)\n",
        "        else:\n",
        "            return None\n",
        "\n",
        "    except Exception as e:\n",
        "        # print(f\"Calc Error: {e}\")\n",
        "        return None\n",
        "\n",
        "def min_max_norm(series):\n",
        "    if len(series) == 0: return series\n",
        "    scaler = MinMaxScaler()\n",
        "    return scaler.fit_transform(series.values.reshape(-1, 1)).flatten()\n",
        "\n",
        "\n",
        "# --- 3. ãƒ¡ã‚¤ãƒ³å‡¦ç† ---\n",
        "print(f\"ğŸš€ è§£æé–‹å§‹ (RRãƒ•ã‚¡ã‚¤ãƒ«å„ªå…ˆãƒ¢ãƒ¼ãƒ‰)\\n\")\n",
        "\n",
        "success_count = 0\n",
        "error_count = 0\n",
        "\n",
        "for subject in subject_list:\n",
        "    for phase in phase_list:\n",
        "        print(f\"[{subject} - {phase}] Processing...\", end=\" \")\n",
        "\n",
        "        # --- A. ãƒ•ã‚¡ã‚¤ãƒ«æ¤œç´¢ ---\n",
        "        # 1. è„³æ³¢ (PLI)\n",
        "        path_eeg = find_file_fuzzy(base_search_path, subject, phase, ['PLI', 'pli'])\n",
        "\n",
        "        # 2. RRé–“éš”ãƒ•ã‚¡ã‚¤ãƒ« (RR, rr, interval)\n",
        "        path_rr = find_file_fuzzy(base_search_path, subject, phase, ['RR', 'rr', 'interval'])\n",
        "\n",
        "        if not path_eeg:\n",
        "            print(f\"âŒ EEGãƒ•ã‚¡ã‚¤ãƒ«ãªã— -> Skip\")\n",
        "            error_count += 1\n",
        "            continue\n",
        "\n",
        "        if not path_rr:\n",
        "            # RRãŒãªã„å ´åˆã€ä¸€å¿œECGã‚’æ¢ã—ã¦ã¿ã‚‹æ•‘æ¸ˆæªç½®\n",
        "            path_rr = find_file_fuzzy(base_search_path, subject, phase, ['ECG', 'ecg'])\n",
        "            is_raw_ecg = True\n",
        "        else:\n",
        "            is_raw_ecg = False # RRãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã£ãŸï¼\n",
        "\n",
        "        if not path_rr:\n",
        "             print(f\"âŒ å¿ƒæ‹ãƒ•ã‚¡ã‚¤ãƒ«ãªã—(RRã‚‚ECGã‚‚) -> Skip\")\n",
        "             error_count += 1\n",
        "             continue\n",
        "\n",
        "        try:\n",
        "            # --- B. èª­ã¿è¾¼ã¿ã¨è¨ˆç®— ---\n",
        "            df_eeg = pd.read_csv(path_eeg)\n",
        "            df_rr = pd.read_csv(path_rr)\n",
        "\n",
        "            # FCz å–å¾—\n",
        "            eeg_data, eeg_col = get_column_data(df_eeg, ['FCz', 'fcz'])\n",
        "            if eeg_data is None:\n",
        "                # ãªã‘ã‚Œã°æœ€åˆã®åˆ—ã‚’ä½¿ã†\n",
        "                eeg_data = df_eeg.iloc[:, 0]\n",
        "                eeg_col = df_eeg.columns[0]\n",
        "\n",
        "            # LF è¨ˆç®—\n",
        "            lf_data = None\n",
        "            lf_label = \"LF\"\n",
        "\n",
        "            if not is_raw_ecg:\n",
        "                # â˜… RRãƒ•ã‚¡ã‚¤ãƒ«ã®å ´åˆ â˜…\n",
        "                # RRåˆ—ã‚’æ¢ã™ (rr, RR, interval, R-R ãªã©)\n",
        "                rr_series, rr_col = get_column_data(df_rr, ['rr', 'RR', 'interval', 'R-R', 'bpm'])\n",
        "\n",
        "                if rr_series is not None:\n",
        "                    # RRé–“éš”ã‹ã‚‰LFã‚’è¨ˆç®—\n",
        "                    # (ã‚‚ã—å˜ä½ãŒç§’ãªã‚‰1000å€ã™ã‚‹å‡¦ç†ã‚’å…¥ã‚Œã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ãŒã€é€šå¸¸msã¨ä»®å®š)\n",
        "                    if rr_series.mean() < 2.0: # å€¤ãŒå°ã•ã„å ´åˆã€Œç§’ã€å˜ä½ã¨ã¿ãªã—ã¦msã«å¤‰æ›\n",
        "                        rr_series = rr_series * 1000\n",
        "\n",
        "                    lf_data = calculate_lf_from_rr(rr_series)\n",
        "                    lf_label = \"LF (from RR)\"\n",
        "                else:\n",
        "                    print(\"âŒ RRãƒ•ã‚¡ã‚¤ãƒ«å†…ã«RRåˆ—ãŒè¦‹å½“ãŸã‚Šã¾ã›ã‚“\", end=\" \")\n",
        "\n",
        "            else:\n",
        "                # â˜… ECGãƒ•ã‚¡ã‚¤ãƒ«ã®å ´åˆ (æ•‘æ¸ˆ) â˜…\n",
        "                # å‰å›ã®ã‚³ãƒ¼ãƒ‰ã¨åŒã˜(çœç•¥å¯ã§ã™ãŒå¿µã®ãŸã‚)\n",
        "                print(\"(RRãªã—: ECGä½¿ç”¨)\", end=\" \")\n",
        "                # ... (ä»Šå›ã¯RRæ¨å¥¨ãªã®ã§ã“ã“ã¯å¤±æ•—ã—ã¦ã‚‚OKã¨ã—ã¾ã™)\n",
        "\n",
        "            if lf_data is None:\n",
        "                print(\"âŒ LFè¨ˆç®—å¤±æ•—\")\n",
        "                error_count += 1\n",
        "                continue\n",
        "\n",
        "            # --- C. æç”» ---\n",
        "            # é•·ã•åˆã‚ã›\n",
        "            min_len = min(len(eeg_data), len(lf_data))\n",
        "            eeg_plot = eeg_data.iloc[:min_len]\n",
        "            lf_plot = lf_data.iloc[:min_len]\n",
        "\n",
        "            # ã‚¹ãƒ ãƒ¼ã‚¸ãƒ³ã‚°\n",
        "            window = 50\n",
        "            eeg_smooth = eeg_plot.rolling(window).mean()\n",
        "            lf_smooth = lf_plot.rolling(window).mean()\n",
        "\n",
        "            fig, ax1 = plt.subplots(figsize=(12, 5))\n",
        "\n",
        "            ax1.plot(min_max_norm(eeg_smooth), color='blue', label=f'EEG: {eeg_col}', alpha=0.7)\n",
        "            ax1.set_ylabel('Normalized EEG', color='blue')\n",
        "\n",
        "            ax2 = ax1.twinx()\n",
        "            ax2.plot(min_max_norm(lf_smooth), color='orange', label=f'HRV: {lf_label}', alpha=0.7)\n",
        "            ax2.set_ylabel('Normalized LF', color='orange')\n",
        "\n",
        "            plt.title(f\"{subject} - {phase}\")\n",
        "            lines1, labels1 = ax1.get_legend_handles_labels()\n",
        "            lines2, labels2 = ax2.get_legend_handles_labels()\n",
        "            ax1.legend(lines1 + lines2, labels1 + labels2, loc='upper left')\n",
        "\n",
        "            save_name = f\"{subject}_{phase}_RR_analysis.png\"\n",
        "            plt.savefig(os.path.join(output_dir, save_name))\n",
        "            plt.close()\n",
        "\n",
        "            print(f\"âœ… ä¿å­˜å®Œäº†\")\n",
        "            success_count += 1\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"âŒ Err: {e}\")\n",
        "            error_count += 1\n",
        "\n",
        "# --- ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ ---\n",
        "print(f\"\\nå®Œäº†: æˆåŠŸ {success_count}, å¤±æ•— {error_count}\")\n",
        "if success_count > 0:\n",
        "    shutil.make_archive('RR_Analysis_Results', 'zip', output_dir)\n",
        "    from google.colab import files\n",
        "    files.download('RR_Analysis_Results.zip')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OEPuq5Z5T1jV",
        "outputId": "8f08c663-718a-4446-9d85-6d616d77cea3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸš€ è§£æé–‹å§‹ (RRãƒ•ã‚¡ã‚¤ãƒ«å„ªå…ˆãƒ¢ãƒ¼ãƒ‰)\n",
            "\n",
            "[ishida - rest] Processing... âŒ RRãƒ•ã‚¡ã‚¤ãƒ«å†…ã«RRåˆ—ãŒè¦‹å½“ãŸã‚Šã¾ã›ã‚“ âŒ LFè¨ˆç®—å¤±æ•—\n",
            "[ishida - boredom] Processing... âŒ RRãƒ•ã‚¡ã‚¤ãƒ«å†…ã«RRåˆ—ãŒè¦‹å½“ãŸã‚Šã¾ã›ã‚“ âŒ LFè¨ˆç®—å¤±æ•—\n",
            "[ishida - flow] Processing... âŒ RRãƒ•ã‚¡ã‚¤ãƒ«å†…ã«RRåˆ—ãŒè¦‹å½“ãŸã‚Šã¾ã›ã‚“ âŒ LFè¨ˆç®—å¤±æ•—\n",
            "[ishida - flow_ultra] Processing... âŒ RRãƒ•ã‚¡ã‚¤ãƒ«å†…ã«RRåˆ—ãŒè¦‹å½“ãŸã‚Šã¾ã›ã‚“ âŒ LFè¨ˆç®—å¤±æ•—\n",
            "[ishida - overload] Processing... âŒ RRãƒ•ã‚¡ã‚¤ãƒ«å†…ã«RRåˆ—ãŒè¦‹å½“ãŸã‚Šã¾ã›ã‚“ âŒ LFè¨ˆç®—å¤±æ•—\n",
            "[yamamoto - rest] Processing... âŒ RRãƒ•ã‚¡ã‚¤ãƒ«å†…ã«RRåˆ—ãŒè¦‹å½“ãŸã‚Šã¾ã›ã‚“ âŒ LFè¨ˆç®—å¤±æ•—\n",
            "[yamamoto - boredom] Processing... âŒ RRãƒ•ã‚¡ã‚¤ãƒ«å†…ã«RRåˆ—ãŒè¦‹å½“ãŸã‚Šã¾ã›ã‚“ âŒ LFè¨ˆç®—å¤±æ•—\n",
            "[yamamoto - flow] Processing... âŒ RRãƒ•ã‚¡ã‚¤ãƒ«å†…ã«RRåˆ—ãŒè¦‹å½“ãŸã‚Šã¾ã›ã‚“ âŒ LFè¨ˆç®—å¤±æ•—\n",
            "[yamamoto - flow_ultra] Processing... âŒ RRãƒ•ã‚¡ã‚¤ãƒ«å†…ã«RRåˆ—ãŒè¦‹å½“ãŸã‚Šã¾ã›ã‚“ âŒ LFè¨ˆç®—å¤±æ•—\n",
            "[yamamoto - overload] Processing... âŒ RRãƒ•ã‚¡ã‚¤ãƒ«å†…ã«RRåˆ—ãŒè¦‹å½“ãŸã‚Šã¾ã›ã‚“ âŒ LFè¨ˆç®—å¤±æ•—\n",
            "[ohashi - rest] Processing... âŒ RRãƒ•ã‚¡ã‚¤ãƒ«å†…ã«RRåˆ—ãŒè¦‹å½“ãŸã‚Šã¾ã›ã‚“ âŒ LFè¨ˆç®—å¤±æ•—\n",
            "[ohashi - boredom] Processing... âŒ RRãƒ•ã‚¡ã‚¤ãƒ«å†…ã«RRåˆ—ãŒè¦‹å½“ãŸã‚Šã¾ã›ã‚“ âŒ LFè¨ˆç®—å¤±æ•—\n",
            "[ohashi - flow] Processing... âŒ RRãƒ•ã‚¡ã‚¤ãƒ«å†…ã«RRåˆ—ãŒè¦‹å½“ãŸã‚Šã¾ã›ã‚“ âŒ LFè¨ˆç®—å¤±æ•—\n",
            "[ohashi - flow_ultra] Processing... âŒ RRãƒ•ã‚¡ã‚¤ãƒ«å†…ã«RRåˆ—ãŒè¦‹å½“ãŸã‚Šã¾ã›ã‚“ âŒ LFè¨ˆç®—å¤±æ•—\n",
            "[ohashi - overload] Processing... âŒ RRãƒ•ã‚¡ã‚¤ãƒ«å†…ã«RRåˆ—ãŒè¦‹å½“ãŸã‚Šã¾ã›ã‚“ âŒ LFè¨ˆç®—å¤±æ•—\n",
            "[miyake - rest] Processing... âŒ RRãƒ•ã‚¡ã‚¤ãƒ«å†…ã«RRåˆ—ãŒè¦‹å½“ãŸã‚Šã¾ã›ã‚“ âŒ LFè¨ˆç®—å¤±æ•—\n",
            "[miyake - boredom] Processing... âŒ RRãƒ•ã‚¡ã‚¤ãƒ«å†…ã«RRåˆ—ãŒè¦‹å½“ãŸã‚Šã¾ã›ã‚“ âŒ LFè¨ˆç®—å¤±æ•—\n",
            "[miyake - flow] Processing... âŒ RRãƒ•ã‚¡ã‚¤ãƒ«å†…ã«RRåˆ—ãŒè¦‹å½“ãŸã‚Šã¾ã›ã‚“ âŒ LFè¨ˆç®—å¤±æ•—\n",
            "[miyake - flow_ultra] Processing... âŒ RRãƒ•ã‚¡ã‚¤ãƒ«å†…ã«RRåˆ—ãŒè¦‹å½“ãŸã‚Šã¾ã›ã‚“ âŒ LFè¨ˆç®—å¤±æ•—\n",
            "[miyake - overload] Processing... âŒ RRãƒ•ã‚¡ã‚¤ãƒ«å†…ã«RRåˆ—ãŒè¦‹å½“ãŸã‚Šã¾ã›ã‚“ âŒ LFè¨ˆç®—å¤±æ•—\n",
            "[mitsuhashi - rest] Processing... âŒ RRãƒ•ã‚¡ã‚¤ãƒ«å†…ã«RRåˆ—ãŒè¦‹å½“ãŸã‚Šã¾ã›ã‚“ âŒ LFè¨ˆç®—å¤±æ•—\n",
            "[mitsuhashi - boredom] Processing... âŒ RRãƒ•ã‚¡ã‚¤ãƒ«å†…ã«RRåˆ—ãŒè¦‹å½“ãŸã‚Šã¾ã›ã‚“ âŒ LFè¨ˆç®—å¤±æ•—\n",
            "[mitsuhashi - flow] Processing... âŒ RRãƒ•ã‚¡ã‚¤ãƒ«å†…ã«RRåˆ—ãŒè¦‹å½“ãŸã‚Šã¾ã›ã‚“ âŒ LFè¨ˆç®—å¤±æ•—\n",
            "[mitsuhashi - flow_ultra] Processing... âŒ RRãƒ•ã‚¡ã‚¤ãƒ«å†…ã«RRåˆ—ãŒè¦‹å½“ãŸã‚Šã¾ã›ã‚“ âŒ LFè¨ˆç®—å¤±æ•—\n",
            "[mitsuhashi - overload] Processing... âŒ RRãƒ•ã‚¡ã‚¤ãƒ«å†…ã«RRåˆ—ãŒè¦‹å½“ãŸã‚Šã¾ã›ã‚“ âŒ LFè¨ˆç®—å¤±æ•—\n",
            "[ko - rest] Processing... âŒ RRãƒ•ã‚¡ã‚¤ãƒ«å†…ã«RRåˆ—ãŒè¦‹å½“ãŸã‚Šã¾ã›ã‚“ âŒ LFè¨ˆç®—å¤±æ•—\n",
            "[ko - boredom] Processing... âŒ RRãƒ•ã‚¡ã‚¤ãƒ«å†…ã«RRåˆ—ãŒè¦‹å½“ãŸã‚Šã¾ã›ã‚“ âŒ LFè¨ˆç®—å¤±æ•—\n",
            "[ko - flow] Processing... âŒ RRãƒ•ã‚¡ã‚¤ãƒ«å†…ã«RRåˆ—ãŒè¦‹å½“ãŸã‚Šã¾ã›ã‚“ âŒ LFè¨ˆç®—å¤±æ•—\n",
            "[ko - flow_ultra] Processing... âŒ RRãƒ•ã‚¡ã‚¤ãƒ«å†…ã«RRåˆ—ãŒè¦‹å½“ãŸã‚Šã¾ã›ã‚“ âŒ LFè¨ˆç®—å¤±æ•—\n",
            "[ko - overload] Processing... âŒ RRãƒ•ã‚¡ã‚¤ãƒ«å†…ã«RRåˆ—ãŒè¦‹å½“ãŸã‚Šã¾ã›ã‚“ âŒ LFè¨ˆç®—å¤±æ•—\n",
            "[hayato - rest] Processing... âŒ RRãƒ•ã‚¡ã‚¤ãƒ«å†…ã«RRåˆ—ãŒè¦‹å½“ãŸã‚Šã¾ã›ã‚“ âŒ LFè¨ˆç®—å¤±æ•—\n",
            "[hayato - boredom] Processing... âŒ RRãƒ•ã‚¡ã‚¤ãƒ«å†…ã«RRåˆ—ãŒè¦‹å½“ãŸã‚Šã¾ã›ã‚“ âŒ LFè¨ˆç®—å¤±æ•—\n",
            "[hayato - flow] Processing... âŒ RRãƒ•ã‚¡ã‚¤ãƒ«å†…ã«RRåˆ—ãŒè¦‹å½“ãŸã‚Šã¾ã›ã‚“ âŒ LFè¨ˆç®—å¤±æ•—\n",
            "[hayato - flow_ultra] Processing... âŒ RRãƒ•ã‚¡ã‚¤ãƒ«å†…ã«RRåˆ—ãŒè¦‹å½“ãŸã‚Šã¾ã›ã‚“ âŒ LFè¨ˆç®—å¤±æ•—\n",
            "[hayato - overload] Processing... âŒ RRãƒ•ã‚¡ã‚¤ãƒ«å†…ã«RRåˆ—ãŒè¦‹å½“ãŸã‚Šã¾ã›ã‚“ âŒ LFè¨ˆç®—å¤±æ•—\n",
            "[chika - rest] Processing... âŒ RRãƒ•ã‚¡ã‚¤ãƒ«å†…ã«RRåˆ—ãŒè¦‹å½“ãŸã‚Šã¾ã›ã‚“ âŒ LFè¨ˆç®—å¤±æ•—\n",
            "[chika - boredom] Processing... âŒ RRãƒ•ã‚¡ã‚¤ãƒ«å†…ã«RRåˆ—ãŒè¦‹å½“ãŸã‚Šã¾ã›ã‚“ âŒ LFè¨ˆç®—å¤±æ•—\n",
            "[chika - flow] Processing... âŒ RRãƒ•ã‚¡ã‚¤ãƒ«å†…ã«RRåˆ—ãŒè¦‹å½“ãŸã‚Šã¾ã›ã‚“ âŒ LFè¨ˆç®—å¤±æ•—\n",
            "[chika - flow_ultra] Processing... âŒ RRãƒ•ã‚¡ã‚¤ãƒ«å†…ã«RRåˆ—ãŒè¦‹å½“ãŸã‚Šã¾ã›ã‚“ âŒ LFè¨ˆç®—å¤±æ•—\n",
            "[chika - overload] Processing... âŒ RRãƒ•ã‚¡ã‚¤ãƒ«å†…ã«RRåˆ—ãŒè¦‹å½“ãŸã‚Šã¾ã›ã‚“ âŒ LFè¨ˆç®—å¤±æ•—\n",
            "[nagomi - rest] Processing... âŒ RRãƒ•ã‚¡ã‚¤ãƒ«å†…ã«RRåˆ—ãŒè¦‹å½“ãŸã‚Šã¾ã›ã‚“ âŒ LFè¨ˆç®—å¤±æ•—\n",
            "[nagomi - boredom] Processing... âŒ RRãƒ•ã‚¡ã‚¤ãƒ«å†…ã«RRåˆ—ãŒè¦‹å½“ãŸã‚Šã¾ã›ã‚“ âŒ LFè¨ˆç®—å¤±æ•—\n",
            "[nagomi - flow] Processing... âŒ RRãƒ•ã‚¡ã‚¤ãƒ«å†…ã«RRåˆ—ãŒè¦‹å½“ãŸã‚Šã¾ã›ã‚“ âŒ LFè¨ˆç®—å¤±æ•—\n",
            "[nagomi - flow_ultra] Processing... âŒ RRãƒ•ã‚¡ã‚¤ãƒ«å†…ã«RRåˆ—ãŒè¦‹å½“ãŸã‚Šã¾ã›ã‚“ âŒ LFè¨ˆç®—å¤±æ•—\n",
            "[nagomi - overload] Processing... âŒ RRãƒ•ã‚¡ã‚¤ãƒ«å†…ã«RRåˆ—ãŒè¦‹å½“ãŸã‚Šã¾ã›ã‚“ âŒ LFè¨ˆç®—å¤±æ•—\n",
            "[mio - rest] Processing... âŒ RRãƒ•ã‚¡ã‚¤ãƒ«å†…ã«RRåˆ—ãŒè¦‹å½“ãŸã‚Šã¾ã›ã‚“ âŒ LFè¨ˆç®—å¤±æ•—\n",
            "[mio - boredom] Processing... âŒ RRãƒ•ã‚¡ã‚¤ãƒ«å†…ã«RRåˆ—ãŒè¦‹å½“ãŸã‚Šã¾ã›ã‚“ âŒ LFè¨ˆç®—å¤±æ•—\n",
            "[mio - flow] Processing... âŒ RRãƒ•ã‚¡ã‚¤ãƒ«å†…ã«RRåˆ—ãŒè¦‹å½“ãŸã‚Šã¾ã›ã‚“ âŒ LFè¨ˆç®—å¤±æ•—\n",
            "[mio - flow_ultra] Processing... âŒ RRãƒ•ã‚¡ã‚¤ãƒ«å†…ã«RRåˆ—ãŒè¦‹å½“ãŸã‚Šã¾ã›ã‚“ âŒ LFè¨ˆç®—å¤±æ•—\n",
            "[mio - overload] Processing... âŒ RRãƒ•ã‚¡ã‚¤ãƒ«å†…ã«RRåˆ—ãŒè¦‹å½“ãŸã‚Šã¾ã›ã‚“ âŒ LFè¨ˆç®—å¤±æ•—\n",
            "\n",
            "å®Œäº†: æˆåŠŸ 0, å¤±æ•— 50\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# ã€RRåˆ—è‡ªå‹•ç‰¹å®šç‰ˆã€‘EEG(FCz) & HRV(LF) è§£æã‚³ãƒ¼ãƒ‰\n",
        "# =============================================================================\n",
        "\n",
        "import sys\n",
        "import os\n",
        "import glob\n",
        "import shutil\n",
        "import warnings\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy import signal\n",
        "from scipy.interpolate import interp1d\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from google.colab import drive\n",
        "\n",
        "# è­¦å‘Šéè¡¨ç¤º\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# --- 1. è¨­å®šã‚¨ãƒªã‚¢ ---\n",
        "\n",
        "if not os.path.exists('/content/drive'):\n",
        "    drive.mount('/content/drive')\n",
        "\n",
        "# è¦ªãƒ•ã‚©ãƒ«ãƒ€ã®ãƒ‘ã‚¹\n",
        "base_search_path = \"/content/drive/MyDrive/csv\"\n",
        "\n",
        "subject_list = [\n",
        "    'ishida', 'yamamoto', 'ohashi', 'miyake','mitsuhashi',\n",
        "    'ko', 'hayato','chika', 'nagomi', 'mio'\n",
        "]\n",
        "\n",
        "# ãƒ•ã‚§ãƒ¼ã‚ºãƒªã‚¹ãƒˆ\n",
        "phase_list = ['rest','boredom', 'flow', 'flow_ultra', 'overload']\n",
        "\n",
        "# çµæœä¿å­˜ãƒ•ã‚©ãƒ«ãƒ€\n",
        "output_dir = \"Final_Graphs_RR_Auto\"\n",
        "if os.path.exists(output_dir):\n",
        "    shutil.rmtree(output_dir)\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "\n",
        "# --- 2. é–¢æ•°ç¾¤ ---\n",
        "\n",
        "def find_file_fuzzy(base_path, subject, phase, file_type_keywords):\n",
        "    \"\"\"\n",
        "    ãƒ•ã‚¡ã‚¤ãƒ«åã«ã€Œåå‰ã€ã€Œãƒ•ã‚§ãƒ¼ã‚ºã€ã€Œã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã€ã‚’å«ã‚€ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¢ã™\n",
        "    \"\"\"\n",
        "    # æ¤œç´¢: *ishida*flow*RR*.csv ã®ã‚ˆã†ãªãƒ‘ã‚¿ãƒ¼ãƒ³\n",
        "    search_pattern = os.path.join(base_path, \"**\", f\"*{subject}*{phase}*.csv\")\n",
        "    candidates = glob.glob(search_pattern, recursive=True)\n",
        "\n",
        "    for path in candidates:\n",
        "        filename = os.path.basename(path)\n",
        "        # æŒ‡å®šã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã®ã„ãšã‚Œã‹ãŒå«ã¾ã‚Œã¦ã„ã‚‹ã‹\n",
        "        if any(k.lower() in filename.lower() for k in file_type_keywords):\n",
        "            return path\n",
        "    return None\n",
        "\n",
        "def get_column_data(df, target_names):\n",
        "    \"\"\"åˆ—åãŒã‚ã„ã¾ã„ã§ã‚‚ãƒ‡ãƒ¼ã‚¿ã‚’å–ã‚Šå‡ºã™\"\"\"\n",
        "    df.columns = [str(c).strip() for c in df.columns]\n",
        "    for col in df.columns:\n",
        "        for target in target_names:\n",
        "            if col.lower() == target.lower():\n",
        "                return df[col], col\n",
        "    return None, None\n",
        "\n",
        "def smart_extract_rr_data(df):\n",
        "    \"\"\"\n",
        "    â˜…ä»Šå›ã®ã‚­ãƒ¢â˜…\n",
        "    åˆ—åãŒåˆ†ã‹ã‚‰ãªãã¦ã‚‚ã€å€¤ã®ç¯„å›²ã‚’è¦‹ã¦RRé–“éš”ãƒ‡ãƒ¼ã‚¿(ms)ã‚’è‡ªå‹•ç‰¹å®šã™ã‚‹\n",
        "    \"\"\"\n",
        "    # 1. ã¾ãšã¯åˆ—åã§æ¢ã™\n",
        "    target_names = ['rr_interval', 'rr', 'RR', 'interval', 'R-R', 'bpm', 'ibi']\n",
        "    series, name = get_column_data(df, target_names)\n",
        "    if series is not None:\n",
        "        return series, name\n",
        "\n",
        "    # 2. è¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã€å…¨åˆ—ã‚’ã‚¹ã‚­ãƒ£ãƒ³ã—ã¦ã€Œå€¤ãŒå¿ƒæ‹ã£ã½ã„åˆ—ã€ã‚’æ¢ã™\n",
        "    # å¿ƒæ‹é–“éš”(ms)ã¯é€šå¸¸ 300ms(å¿ƒæ‹200) ï½ 1500ms(å¿ƒæ‹40) ã®é–“ã«ã‚ã‚‹ã¯ãš\n",
        "    for col in df.columns:\n",
        "        try:\n",
        "            # æ•°å€¤ãƒ‡ãƒ¼ã‚¿ã®ã¿æŠ½å‡º\n",
        "            col_data = pd.to_numeric(df[col], errors='coerce').dropna()\n",
        "            if len(col_data) < 10: continue\n",
        "\n",
        "            mean_val = col_data.mean()\n",
        "\n",
        "            # ãƒŸãƒªç§’å˜ä½ (300 ~ 1500)\n",
        "            if 300 < mean_val < 1500:\n",
        "                return col_data, f\"{col} (Auto-detected ms)\"\n",
        "\n",
        "            # ç§’å˜ä½ (0.3 ~ 1.5) -> 1000å€ã—ã¦è¿”ã™\n",
        "            if 0.3 < mean_val < 1.5:\n",
        "                return col_data * 1000, f\"{col} (Auto-detected sec)\"\n",
        "\n",
        "        except:\n",
        "            continue\n",
        "\n",
        "    return None, None\n",
        "\n",
        "def calculate_lf_from_rr(rr_data_ms):\n",
        "    \"\"\"RRé–“éš”(ms)ã‹ã‚‰LFæˆåˆ†ã®æ™‚ç³»åˆ—å¤‰åŒ–ã‚’è¨ˆç®—\"\"\"\n",
        "    try:\n",
        "        # å¤–ã‚Œå€¤é™¤å»\n",
        "        rr_data = rr_data_ms[(rr_data_ms > 300) & (rr_data_ms < 2000)]\n",
        "        if len(rr_data) < 10: return None\n",
        "\n",
        "        # æ™‚ç³»åˆ—ã¸ã®å¤‰æ› (è£œé–“)\n",
        "        t_cumulative = np.cumsum(rr_data) / 1000.0\n",
        "        t_cumulative = t_cumulative - t_cumulative.iloc[0]\n",
        "\n",
        "        fs_interp = 4.0\n",
        "        t_interp = np.arange(0, t_cumulative.iloc[-1], 1/fs_interp)\n",
        "\n",
        "        f_interp = interp1d(t_cumulative, rr_data, kind='cubic', fill_value=\"extrapolate\")\n",
        "        rr_interp = f_interp(t_interp)\n",
        "\n",
        "        # LFãƒ‘ãƒ¯ãƒ¼è¨ˆç®— (Window 60s)\n",
        "        window_sec = 60\n",
        "        step_sec = 10\n",
        "        nperseg = int(window_sec * fs_interp)\n",
        "        step_samples = int(step_sec * fs_interp)\n",
        "\n",
        "        if len(rr_interp) < nperseg: return None # ãƒ‡ãƒ¼ã‚¿ãŒçŸ­ã™ãã‚‹\n",
        "\n",
        "        lf_series = []\n",
        "        time_series = []\n",
        "\n",
        "        for i in range(0, len(rr_interp) - nperseg, step_samples):\n",
        "            segment = rr_interp[i : i + nperseg]\n",
        "            freqs, psd = signal.welch(segment, fs=fs_interp, nperseg=nperseg)\n",
        "\n",
        "            # LFå¸¯åŸŸ (0.04 - 0.15 Hz)\n",
        "            lf_band = (freqs >= 0.04) & (freqs <= 0.15)\n",
        "            lf_power = np.trapz(psd[lf_band], freqs[lf_band])\n",
        "\n",
        "            lf_series.append(lf_power)\n",
        "            current_time = t_interp[i + nperseg//2]\n",
        "            time_series.append(current_time)\n",
        "\n",
        "        # ãƒ‡ãƒ¼ã‚¿å¼•ãä¼¸ã°ã—\n",
        "        x_target = np.linspace(0, t_cumulative.iloc[-1], len(rr_data_ms))\n",
        "        if len(time_series) > 1:\n",
        "            f_resample = interp1d(time_series, lf_series, kind='linear', fill_value=\"extrapolate\")\n",
        "            lf_final = f_resample(x_target)\n",
        "            return pd.Series(lf_final, index=rr_data_ms.index)\n",
        "        else:\n",
        "            return None\n",
        "\n",
        "    except Exception as e:\n",
        "        return None\n",
        "\n",
        "def min_max_norm(series):\n",
        "    if len(series) == 0: return series\n",
        "    scaler = MinMaxScaler()\n",
        "    return scaler.fit_transform(series.values.reshape(-1, 1)).flatten()\n",
        "\n",
        "\n",
        "# --- 3. ãƒ¡ã‚¤ãƒ³å‡¦ç† ---\n",
        "print(f\"ğŸš€ è§£æé–‹å§‹ (RRè‡ªå‹•ç‰¹å®šãƒ¢ãƒ¼ãƒ‰)\\n\")\n",
        "\n",
        "success_count = 0\n",
        "error_count = 0\n",
        "\n",
        "for subject in subject_list:\n",
        "    for phase in phase_list:\n",
        "        print(f\"[{subject} - {phase}] Processing...\", end=\" \")\n",
        "\n",
        "        # --- A. ãƒ•ã‚¡ã‚¤ãƒ«æ¤œç´¢ ---\n",
        "        path_eeg = find_file_fuzzy(base_search_path, subject, phase, ['PLI', 'pli'])\n",
        "        # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã« 'rr_interval' ã‚’è¿½åŠ \n",
        "        path_rr = find_file_fuzzy(base_search_path, subject, phase, ['rr_interval', 'RR', 'rr', 'interval'])\n",
        "\n",
        "        if not path_eeg or not path_rr:\n",
        "            print(f\"âŒ ãƒ•ã‚¡ã‚¤ãƒ«æ¬ æ (EEG:{'ã€‡' if path_eeg else 'Ã—'}, RR:{'ã€‡' if path_rr else 'Ã—'}) -> Skip\")\n",
        "            error_count += 1\n",
        "            continue\n",
        "\n",
        "        try:\n",
        "            # --- B. èª­ã¿è¾¼ã¿ ---\n",
        "            df_eeg = pd.read_csv(path_eeg)\n",
        "            df_rr = pd.read_csv(path_rr)\n",
        "\n",
        "            # FCz å–å¾—\n",
        "            eeg_data, eeg_col = get_column_data(df_eeg, ['FCz', 'fcz'])\n",
        "            if eeg_data is None:\n",
        "                eeg_data = df_eeg.iloc[:, 0] # ãªã‘ã‚Œã°1åˆ—ç›®\n",
        "                eeg_col = df_eeg.columns[0]\n",
        "\n",
        "            # LF (RR) å–å¾— â˜…ã“ã“ãŒå¤‰ã‚ã£ãŸï¼â˜…\n",
        "            rr_series, rr_col_name = smart_extract_rr_data(df_rr)\n",
        "\n",
        "            if rr_series is None:\n",
        "                print(f\"âŒ RRãƒ‡ãƒ¼ã‚¿ç‰¹å®šä¸èƒ½ (Columns: {list(df_rr.columns)})\")\n",
        "                error_count += 1\n",
        "                continue\n",
        "\n",
        "            # LFè¨ˆç®—\n",
        "            lf_data = calculate_lf_from_rr(rr_series)\n",
        "\n",
        "            if lf_data is None:\n",
        "                print(\"âŒ LFè¨ˆç®—å¤±æ•—(ãƒ‡ãƒ¼ã‚¿çŸ­/ãƒã‚¤ã‚º)\")\n",
        "                error_count += 1\n",
        "                continue\n",
        "\n",
        "            # --- C. æç”» ---\n",
        "            min_len = min(len(eeg_data), len(lf_data))\n",
        "            eeg_plot = eeg_data.iloc[:min_len]\n",
        "            lf_plot = lf_data.iloc[:min_len]\n",
        "\n",
        "            # ã‚¹ãƒ ãƒ¼ã‚¸ãƒ³ã‚°\n",
        "            window = 50\n",
        "            eeg_smooth = eeg_plot.rolling(window).mean()\n",
        "            lf_smooth = lf_plot.rolling(window).mean()\n",
        "\n",
        "            fig, ax1 = plt.subplots(figsize=(12, 5))\n",
        "\n",
        "            ax1.plot(min_max_norm(eeg_smooth), color='blue', label=f'EEG: {eeg_col}', alpha=0.7)\n",
        "            ax1.set_ylabel('Normalized EEG', color='blue')\n",
        "\n",
        "            ax2 = ax1.twinx()\n",
        "            ax2.plot(min_max_norm(lf_smooth), color='orange', label=f'HRV: LF (from {rr_col_name})', alpha=0.7)\n",
        "            ax2.set_ylabel('Normalized LF', color='orange')\n",
        "\n",
        "            plt.title(f\"{subject} - {phase}\")\n",
        "            lines1, labels1 = ax1.get_legend_handles_labels()\n",
        "            lines2, labels2 = ax2.get_legend_handles_labels()\n",
        "            ax1.legend(lines1 + lines2, labels1 + labels2, loc='upper left')\n",
        "\n",
        "            save_name = f\"{subject}_{phase}_RR_analysis.png\"\n",
        "            plt.savefig(os.path.join(output_dir, save_name))\n",
        "            plt.close()\n",
        "\n",
        "            print(f\"âœ… ä¿å­˜å®Œäº†\")\n",
        "            success_count += 1\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"âŒ Err: {e}\")\n",
        "            error_count += 1\n",
        "\n",
        "# --- ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ ---\n",
        "print(f\"\\nå®Œäº†: æˆåŠŸ {success_count}, å¤±æ•— {error_count}\")\n",
        "if success_count > 0:\n",
        "    shutil.make_archive('Results_RR_Auto', 'zip', output_dir)\n",
        "    from google.colab import files\n",
        "    files.download('Results_RR_Auto.zip')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 935
        },
        "id": "GYd6lYYuUqNt",
        "outputId": "5e043ee1-f39e-4046-817b-6fadca90a4e6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸš€ è§£æé–‹å§‹ (RRè‡ªå‹•ç‰¹å®šãƒ¢ãƒ¼ãƒ‰)\n",
            "\n",
            "[ishida - rest] Processing... âœ… ä¿å­˜å®Œäº†\n",
            "[ishida - boredom] Processing... âœ… ä¿å­˜å®Œäº†\n",
            "[ishida - flow] Processing... âœ… ä¿å­˜å®Œäº†\n",
            "[ishida - flow_ultra] Processing... âœ… ä¿å­˜å®Œäº†\n",
            "[ishida - overload] Processing... âœ… ä¿å­˜å®Œäº†\n",
            "[yamamoto - rest] Processing... âœ… ä¿å­˜å®Œäº†\n",
            "[yamamoto - boredom] Processing... âœ… ä¿å­˜å®Œäº†\n",
            "[yamamoto - flow] Processing... âœ… ä¿å­˜å®Œäº†\n",
            "[yamamoto - flow_ultra] Processing... âœ… ä¿å­˜å®Œäº†\n",
            "[yamamoto - overload] Processing... âœ… ä¿å­˜å®Œäº†\n",
            "[ohashi - rest] Processing... âœ… ä¿å­˜å®Œäº†\n",
            "[ohashi - boredom] Processing... âœ… ä¿å­˜å®Œäº†\n",
            "[ohashi - flow] Processing... âœ… ä¿å­˜å®Œäº†\n",
            "[ohashi - flow_ultra] Processing... âœ… ä¿å­˜å®Œäº†\n",
            "[ohashi - overload] Processing... âœ… ä¿å­˜å®Œäº†\n",
            "[miyake - rest] Processing... âœ… ä¿å­˜å®Œäº†\n",
            "[miyake - boredom] Processing... âœ… ä¿å­˜å®Œäº†\n",
            "[miyake - flow] Processing... âœ… ä¿å­˜å®Œäº†\n",
            "[miyake - flow_ultra] Processing... âœ… ä¿å­˜å®Œäº†\n",
            "[miyake - overload] Processing... âœ… ä¿å­˜å®Œäº†\n",
            "[mitsuhashi - rest] Processing... âœ… ä¿å­˜å®Œäº†\n",
            "[mitsuhashi - boredom] Processing... âœ… ä¿å­˜å®Œäº†\n",
            "[mitsuhashi - flow] Processing... âœ… ä¿å­˜å®Œäº†\n",
            "[mitsuhashi - flow_ultra] Processing... âœ… ä¿å­˜å®Œäº†\n",
            "[mitsuhashi - overload] Processing... âœ… ä¿å­˜å®Œäº†\n",
            "[ko - rest] Processing... âœ… ä¿å­˜å®Œäº†\n",
            "[ko - boredom] Processing... âœ… ä¿å­˜å®Œäº†\n",
            "[ko - flow] Processing... âœ… ä¿å­˜å®Œäº†\n",
            "[ko - flow_ultra] Processing... âœ… ä¿å­˜å®Œäº†\n",
            "[ko - overload] Processing... âœ… ä¿å­˜å®Œäº†\n",
            "[hayato - rest] Processing... âœ… ä¿å­˜å®Œäº†\n",
            "[hayato - boredom] Processing... âœ… ä¿å­˜å®Œäº†\n",
            "[hayato - flow] Processing... âœ… ä¿å­˜å®Œäº†\n",
            "[hayato - flow_ultra] Processing... âœ… ä¿å­˜å®Œäº†\n",
            "[hayato - overload] Processing... âœ… ä¿å­˜å®Œäº†\n",
            "[chika - rest] Processing... âœ… ä¿å­˜å®Œäº†\n",
            "[chika - boredom] Processing... âœ… ä¿å­˜å®Œäº†\n",
            "[chika - flow] Processing... âœ… ä¿å­˜å®Œäº†\n",
            "[chika - flow_ultra] Processing... âœ… ä¿å­˜å®Œäº†\n",
            "[chika - overload] Processing... âœ… ä¿å­˜å®Œäº†\n",
            "[nagomi - rest] Processing... âœ… ä¿å­˜å®Œäº†\n",
            "[nagomi - boredom] Processing... âœ… ä¿å­˜å®Œäº†\n",
            "[nagomi - flow] Processing... âœ… ä¿å­˜å®Œäº†\n",
            "[nagomi - flow_ultra] Processing... âœ… ä¿å­˜å®Œäº†\n",
            "[nagomi - overload] Processing... âœ… ä¿å­˜å®Œäº†\n",
            "[mio - rest] Processing... âœ… ä¿å­˜å®Œäº†\n",
            "[mio - boredom] Processing... âœ… ä¿å­˜å®Œäº†\n",
            "[mio - flow] Processing... âœ… ä¿å­˜å®Œäº†\n",
            "[mio - flow_ultra] Processing... âœ… ä¿å­˜å®Œäº†\n",
            "[mio - overload] Processing... âœ… ä¿å­˜å®Œäº†\n",
            "\n",
            "å®Œäº†: æˆåŠŸ 50, å¤±æ•— 0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_45c76100-2f9a-4018-8aee-7f6012f171c0\", \"Results_RR_Auto.zip\", 2270963)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "ã‚®ã‚¶ã‚®ã‚¶"
      ],
      "metadata": {
        "id": "CzkMDeMtVgol"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# ã€ã‚®ã‚¶ã‚®ã‚¶è¡¨ç¤ºç‰ˆã€‘EEG(FCz) & HRV(LF) è§£æã‚³ãƒ¼ãƒ‰\n",
        "# =============================================================================\n",
        "# å¤‰æ›´ç‚¹: ã‚¹ãƒ ãƒ¼ã‚¸ãƒ³ã‚°(ç§»å‹•å¹³å‡)ã‚’ã»ã¼ç„¡ãã—ã€ãƒ‡ãƒ¼ã‚¿ã®ã€Œã‚®ã‚¶ã‚®ã‚¶æ„Ÿã€ã‚’å‡ºã—ã¾ã™\n",
        "# =============================================================================\n",
        "\n",
        "import sys\n",
        "import os\n",
        "import glob\n",
        "import shutil\n",
        "import warnings\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy import signal\n",
        "from scipy.interpolate import interp1d\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from google.colab import drive\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# 1. è¨­å®šã‚¨ãƒªã‚¢\n",
        "# ---------------------------------------------------------\n",
        "\n",
        "if not os.path.exists('/content/drive'):\n",
        "    drive.mount('/content/drive')\n",
        "\n",
        "base_search_path = \"/content/drive/MyDrive/csv\"\n",
        "\n",
        "\n",
        "# â˜…ã“ã“ãŒèª¿æ•´ãƒã‚¤ãƒ³ãƒˆï¼\n",
        "# 1ãªã‚‰å®Œå…¨ãªç”Ÿãƒ‡ãƒ¼ã‚¿(ã‚®ã‚¶ã‚®ã‚¶æœ€å¼·)ã€‚æ•°å€¤ã‚’ä¸Šã’ã‚‹ã¨æ»‘ã‚‰ã‹ã«ãªã‚Šã¾ã™ã€‚\n",
        "# ãŠæ‰‹æœ¬ã®ã‚ˆã†ãªã‚®ã‚¶ã‚®ã‚¶ãªã‚‰ 3~5 ãã‚‰ã„ãŒé©å½“ã§ã™ã€‚\n",
        "SMOOTHING_WINDOW = 3\n",
        "\n",
        "# å‡ºåŠ›å…ˆ\n",
        "output_dir = \"Graphs_Jagged_View\"\n",
        "if os.path.exists(output_dir):\n",
        "    shutil.rmtree(output_dir)\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "subject_list = [\n",
        "    'ishida', 'yamamoto', 'ohashi', 'miyake','mitsuhashi',\n",
        "    'ko', 'hayato','chika', 'nagomi', 'mio'\n",
        "]\n",
        "\n",
        "# ãƒ•ã‚§ãƒ¼ã‚ºãƒªã‚¹ãƒˆ\n",
        "phase_list = ['rest','boredom', 'flow', 'flow_ultra', 'overload']\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# 2. é–¢æ•°ç¾¤ (å‰å›ã¨åŒã˜å¼·åŠ›ãªåˆ¤å®šãƒ­ã‚¸ãƒƒã‚¯)\n",
        "# ---------------------------------------------------------\n",
        "\n",
        "def find_file_fuzzy(base_path, subject, phase, keywords):\n",
        "    search_pattern = os.path.join(base_path, \"**\", f\"*{subject}*{phase}*.csv\")\n",
        "    candidates = glob.glob(search_pattern, recursive=True)\n",
        "    for path in candidates:\n",
        "        if any(k.lower() in os.path.basename(path).lower() for k in keywords):\n",
        "            return path\n",
        "    return None\n",
        "\n",
        "def get_column_data(df, target_names):\n",
        "    df.columns = [str(c).strip() for c in df.columns]\n",
        "    for col in df.columns:\n",
        "        for target in target_names:\n",
        "            if col.lower() == target.lower():\n",
        "                return df[col], col\n",
        "    return None, None\n",
        "\n",
        "def smart_extract_rr_data(df):\n",
        "    # åå‰ã§æ¢ã™\n",
        "    series, name = get_column_data(df, ['rr_interval', 'rr', 'RR', 'interval', 'R-R', 'bpm'])\n",
        "    if series is not None: return series, name\n",
        "\n",
        "    # å€¤ã§æ¢ã™ (300ms ~ 1500ms)\n",
        "    for col in df.columns:\n",
        "        try:\n",
        "            val = pd.to_numeric(df[col], errors='coerce').dropna()\n",
        "            if len(val) < 10: continue\n",
        "            mean_val = val.mean()\n",
        "            if 300 < mean_val < 1500: return val, f\"{col} (Auto ms)\"\n",
        "            if 0.3 < mean_val < 1.5: return val * 1000, f\"{col} (Auto sec)\"\n",
        "        except: continue\n",
        "    return None, None\n",
        "\n",
        "def calculate_lf_from_rr(rr_data_ms):\n",
        "    try:\n",
        "        # å¤–ã‚Œå€¤é™¤å»\n",
        "        rr_data = rr_data_ms[(rr_data_ms > 300) & (rr_data_ms < 2000)]\n",
        "        if len(rr_data) < 10: return None\n",
        "\n",
        "        # æ™‚é–“è»¸ä½œæˆ\n",
        "        t_cum = np.cumsum(rr_data) / 1000.0\n",
        "        t_cum = t_cum - t_cum.iloc[0]\n",
        "\n",
        "        # ç­‰é–“éš”ãƒªã‚µãƒ³ãƒ—ãƒªãƒ³ã‚° (4Hz)\n",
        "        fs_interp = 4.0\n",
        "        t_interp = np.arange(0, t_cum.iloc[-1], 1/fs_interp)\n",
        "        f_int = interp1d(t_cum, rr_data, kind='linear', fill_value=\"extrapolate\") # cubicã‚ˆã‚Šlinearã®æ–¹ãŒé‹­ããªã‚‹\n",
        "        rr_interp = f_int(t_interp)\n",
        "\n",
        "        # LFè¨ˆç®— (Window 60s, Step 5s -> ã‚¹ãƒ†ãƒƒãƒ—ã‚’ç´°ã‹ãã—ã¦å¤‰å‹•ã‚’æ‰ãˆã‚‹)\n",
        "        window_sec = 60\n",
        "        step_sec = 5 # 10ã‹ã‚‰5ã«çŸ­ç¸®\n",
        "        nperseg = int(window_sec * fs_interp)\n",
        "        step_samples = int(step_sec * fs_interp)\n",
        "\n",
        "        if len(rr_interp) < nperseg: return None\n",
        "\n",
        "        lf_vals = []\n",
        "        t_vals = []\n",
        "\n",
        "        for i in range(0, len(rr_interp) - nperseg, step_samples):\n",
        "            segment = rr_interp[i : i + nperseg]\n",
        "            freqs, psd = signal.welch(segment, fs=fs_interp, nperseg=nperseg)\n",
        "            lf_pow = np.trapz(psd[(freqs >= 0.04) & (freqs <= 0.15)], freqs[(freqs >= 0.04) & (freqs <= 0.15)])\n",
        "            lf_vals.append(lf_pow)\n",
        "            t_vals.append(t_interp[i + nperseg//2])\n",
        "\n",
        "        # è£œé–“ã—ã¦å…ƒã®é•·ã•ã«æˆ»ã™\n",
        "        x_target = np.linspace(0, t_cum.iloc[-1], len(rr_data_ms))\n",
        "        if len(t_vals) > 1:\n",
        "            f_res = interp1d(t_vals, lf_vals, kind='linear', fill_value=\"extrapolate\")\n",
        "            return pd.Series(f_res(x_target), index=rr_data_ms.index)\n",
        "        return None\n",
        "    except: return None\n",
        "\n",
        "def min_max_norm(series):\n",
        "    if len(series) == 0: return series\n",
        "    scaler = MinMaxScaler()\n",
        "    return scaler.fit_transform(series.values.reshape(-1, 1)).flatten()\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# 3. ãƒ¡ã‚¤ãƒ³å‡¦ç†\n",
        "# ---------------------------------------------------------\n",
        "print(f\"ğŸš€ è§£æé–‹å§‹ (ã‚®ã‚¶ã‚®ã‚¶è¡¨ç¤ºãƒ¢ãƒ¼ãƒ‰)\\n\")\n",
        "\n",
        "success_count = 0\n",
        "\n",
        "for subject in subject_list:\n",
        "    for phase in phase_list:\n",
        "        print(f\"[{subject} - {phase}]\", end=\" \")\n",
        "\n",
        "        # ãƒ•ã‚¡ã‚¤ãƒ«æ¤œç´¢\n",
        "        path_eeg = find_file_fuzzy(base_search_path, subject, phase, ['PLI', 'pli'])\n",
        "        path_rr = find_file_fuzzy(base_search_path, subject, phase, ['rr_interval', 'RR', 'rr', 'interval'])\n",
        "\n",
        "        if not path_eeg or not path_rr:\n",
        "            print(\"-> Skip (File Missing)\")\n",
        "            continue\n",
        "\n",
        "        try:\n",
        "            df_eeg = pd.read_csv(path_eeg)\n",
        "            df_rr = pd.read_csv(path_rr)\n",
        "\n",
        "            # ãƒ‡ãƒ¼ã‚¿å–å¾—\n",
        "            eeg_data, eeg_col = get_column_data(df_eeg, ['FCz', 'fcz'])\n",
        "            rr_vals, rr_name = smart_extract_rr_data(df_rr)\n",
        "\n",
        "            if eeg_data is None or rr_vals is None:\n",
        "                print(\"-> Skip (Data Column Missing)\")\n",
        "                continue\n",
        "\n",
        "            eeg_data = eeg_data.fillna(method='ffill').fillna(method='bfill') # æ¬ æåŸ‹ã‚\n",
        "\n",
        "            # LFè¨ˆç®—\n",
        "            lf_data = calculate_lf_from_rr(rr_vals)\n",
        "            if lf_data is None:\n",
        "                print(\"-> LF Calc Failed\")\n",
        "                continue\n",
        "\n",
        "            # æç”»æº–å‚™\n",
        "            min_len = min(len(eeg_data), len(lf_data))\n",
        "            eeg_plot = eeg_data.iloc[:min_len]\n",
        "            lf_plot = lf_data.iloc[:min_len]\n",
        "\n",
        "            # â˜…ã“ã“ã§ã‚¹ãƒ ãƒ¼ã‚¸ãƒ³ã‚°èª¿æ•´\n",
        "            eeg_vis = eeg_plot.rolling(window=SMOOTHING_WINDOW).mean()\n",
        "            lf_vis = lf_plot.rolling(window=SMOOTHING_WINDOW).mean()\n",
        "\n",
        "            # ãƒ—ãƒ­ãƒƒãƒˆ\n",
        "            fig, ax1 = plt.subplots(figsize=(12, 5))\n",
        "\n",
        "            # ç·šã‚’ç´°ã(linewidth=1.0)ã—ã¦ã‚®ã‚¶ã‚®ã‚¶ã‚’éš›ç«‹ãŸã›ã‚‹\n",
        "            ax1.plot(min_max_norm(eeg_vis), color='blue', label='EEG (FCz)', alpha=0.8, linewidth=1.0)\n",
        "            ax1.set_ylabel('EEG Power', color='blue')\n",
        "\n",
        "            ax2 = ax1.twinx()\n",
        "            ax2.plot(min_max_norm(lf_vis), color='orange', label='HRV (LF)', alpha=0.8, linewidth=1.5)\n",
        "            ax2.set_ylabel('LF Power', color='orange')\n",
        "\n",
        "            plt.title(f\"{subject} - {phase}\")\n",
        "            lines1, labels1 = ax1.get_legend_handles_labels()\n",
        "            lines2, labels2 = ax2.get_legend_handles_labels()\n",
        "            ax1.legend(lines1 + lines2, labels1 + labels2, loc='upper left')\n",
        "\n",
        "            out_path = os.path.join(output_dir, f\"{subject}_{phase}.png\")\n",
        "            plt.savefig(out_path)\n",
        "            plt.close()\n",
        "\n",
        "            print(\"âœ… OK\")\n",
        "            success_count += 1\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"-> Error: {e}\")\n",
        "\n",
        "# ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰\n",
        "if success_count > 0:\n",
        "    shutil.make_archive('Results_Jagged', 'zip', output_dir)\n",
        "    from google.colab import files\n",
        "    files.download('Results_Jagged.zip')\n",
        "    print(\"\\nå®Œäº†ï¼ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã•ã‚ŒãŸZipã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚\")\n",
        "else:\n",
        "    print(\"\\nä½œæˆã•ã‚ŒãŸç”»åƒã¯ã‚ã‚Šã¾ã›ã‚“ã€‚\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 935
        },
        "id": "UY4NnlaxVfGo",
        "outputId": "d78c0df3-9bdd-4249-db11-a52ba81be013"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸš€ è§£æé–‹å§‹ (ã‚®ã‚¶ã‚®ã‚¶è¡¨ç¤ºãƒ¢ãƒ¼ãƒ‰)\n",
            "\n",
            "[ishida - rest] âœ… OK\n",
            "[ishida - boredom] âœ… OK\n",
            "[ishida - flow] âœ… OK\n",
            "[ishida - flow_ultra] âœ… OK\n",
            "[ishida - overload] âœ… OK\n",
            "[yamamoto - rest] âœ… OK\n",
            "[yamamoto - boredom] âœ… OK\n",
            "[yamamoto - flow] âœ… OK\n",
            "[yamamoto - flow_ultra] âœ… OK\n",
            "[yamamoto - overload] âœ… OK\n",
            "[ohashi - rest] âœ… OK\n",
            "[ohashi - boredom] âœ… OK\n",
            "[ohashi - flow] âœ… OK\n",
            "[ohashi - flow_ultra] âœ… OK\n",
            "[ohashi - overload] âœ… OK\n",
            "[miyake - rest] âœ… OK\n",
            "[miyake - boredom] âœ… OK\n",
            "[miyake - flow] âœ… OK\n",
            "[miyake - flow_ultra] âœ… OK\n",
            "[miyake - overload] âœ… OK\n",
            "[mitsuhashi - rest] âœ… OK\n",
            "[mitsuhashi - boredom] âœ… OK\n",
            "[mitsuhashi - flow] âœ… OK\n",
            "[mitsuhashi - flow_ultra] âœ… OK\n",
            "[mitsuhashi - overload] âœ… OK\n",
            "[ko - rest] âœ… OK\n",
            "[ko - boredom] âœ… OK\n",
            "[ko - flow] âœ… OK\n",
            "[ko - flow_ultra] âœ… OK\n",
            "[ko - overload] âœ… OK\n",
            "[hayato - rest] âœ… OK\n",
            "[hayato - boredom] âœ… OK\n",
            "[hayato - flow] âœ… OK\n",
            "[hayato - flow_ultra] âœ… OK\n",
            "[hayato - overload] âœ… OK\n",
            "[chika - rest] âœ… OK\n",
            "[chika - boredom] âœ… OK\n",
            "[chika - flow] âœ… OK\n",
            "[chika - flow_ultra] âœ… OK\n",
            "[chika - overload] âœ… OK\n",
            "[nagomi - rest] âœ… OK\n",
            "[nagomi - boredom] âœ… OK\n",
            "[nagomi - flow] âœ… OK\n",
            "[nagomi - flow_ultra] âœ… OK\n",
            "[nagomi - overload] âœ… OK\n",
            "[mio - rest] âœ… OK\n",
            "[mio - boredom] âœ… OK\n",
            "[mio - flow] âœ… OK\n",
            "[mio - flow_ultra] âœ… OK\n",
            "[mio - overload] âœ… OK\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_68b92f65-10c4-484d-b977-2fc9e5fef28d\", \"Results_Jagged.zip\", 2576213)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "å®Œäº†ï¼ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã•ã‚ŒãŸZipã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "ã‚®ã‚¶ã‚®ã‚¶ï¼’\n"
      ],
      "metadata": {
        "id": "kbY8AysoWRJS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# ã€å®Œå…¨ç”Ÿãƒ‡ãƒ¼ã‚¿ãƒ»é«˜è§£åƒåº¦ç‰ˆã€‘EEG & HRV(LF) è§£æã‚³ãƒ¼ãƒ‰\n",
        "# =============================================================================\n",
        "# ç‰¹å¾´:\n",
        "# 1. å¹³å‡åŒ–å‡¦ç†(ã‚¹ãƒ ãƒ¼ã‚¸ãƒ³ã‚°)ã‚’ä¸€åˆ‡è¡Œã„ã¾ã›ã‚“ã€‚\n",
        "# 2. LFã‚’ã€Œ1ç§’åˆ»ã¿ã€ã§ç´°ã‹ãè¨ˆç®—ã—ã€å¤‰å‹•ã‚’ãƒ€ã‚¤ãƒ¬ã‚¯ãƒˆã«å¯è¦–åŒ–ã—ã¾ã™ã€‚\n",
        "# 3. æŒ‡å®šã•ã‚ŒãŸæœ€æ–°ã®å‚åŠ è€…ãƒªã‚¹ãƒˆãƒ»ãƒ•ã‚§ãƒ¼ã‚ºãƒªã‚¹ãƒˆã‚’ä½¿ç”¨ã—ã¾ã™ã€‚\n",
        "# =============================================================================\n",
        "\n",
        "import sys\n",
        "import os\n",
        "import glob\n",
        "import shutil\n",
        "import warnings\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy import signal\n",
        "from scipy.interpolate import interp1d\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from google.colab import drive\n",
        "\n",
        "# è­¦å‘Šéè¡¨ç¤º\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# 1. è¨­å®šã‚¨ãƒªã‚¢ (æŒ‡å®šãƒªã‚¹ãƒˆåæ˜ æ¸ˆã¿)\n",
        "# ---------------------------------------------------------\n",
        "\n",
        "if not os.path.exists('/content/drive'):\n",
        "    drive.mount('/content/drive')\n",
        "\n",
        "base_search_path = \"/content/drive/MyDrive/csv\"\n",
        "\n",
        "# â˜…æŒ‡å®šã•ã‚ŒãŸãƒªã‚¹ãƒˆã«æ›´æ–°ã—ã¾ã—ãŸ\n",
        "subject_list = [\n",
        "    'ishida', 'yamamoto', 'ohashi', 'miyake', 'mitsuhashi',\n",
        "    'ko', 'hayato', 'chika', 'nagomi', 'mio'\n",
        "]\n",
        "\n",
        "# â˜…æŒ‡å®šã•ã‚ŒãŸãƒ•ã‚§ãƒ¼ã‚ºã«æ›´æ–°ã—ã¾ã—ãŸ\n",
        "phase_list = ['rest', 'boredom', 'flow', 'flow_ultra', 'overload']\n",
        "\n",
        "# å‡ºåŠ›å…ˆ\n",
        "output_dir = \"Graphs_NoSmoothing_Raw\"\n",
        "if os.path.exists(output_dir):\n",
        "    shutil.rmtree(output_dir)\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# 2. é–¢æ•°ç¾¤\n",
        "# ---------------------------------------------------------\n",
        "\n",
        "def find_file_fuzzy(base_path, subject, phase, keywords):\n",
        "    \"\"\"ãƒ•ã‚¡ã‚¤ãƒ«æ¤œç´¢ (éƒ¨åˆ†ä¸€è‡´)\"\"\"\n",
        "    search_pattern = os.path.join(base_path, \"**\", f\"*{subject}*{phase}*.csv\")\n",
        "    candidates = glob.glob(search_pattern, recursive=True)\n",
        "    for path in candidates:\n",
        "        if any(k.lower() in os.path.basename(path).lower() for k in keywords):\n",
        "            return path\n",
        "    return None\n",
        "\n",
        "def get_column_data(df, target_names):\n",
        "    \"\"\"åˆ—åæ¤œç´¢\"\"\"\n",
        "    df.columns = [str(c).strip() for c in df.columns]\n",
        "    for col in df.columns:\n",
        "        for target in target_names:\n",
        "            if col.lower() == target.lower():\n",
        "                return df[col], col\n",
        "    return None, None\n",
        "\n",
        "def smart_extract_rr_data(df):\n",
        "    \"\"\"RRé–“éš”ãƒ‡ãƒ¼ã‚¿ã®è‡ªå‹•æŠ½å‡º\"\"\"\n",
        "    # åå‰ã§æ¢ã™\n",
        "    series, name = get_column_data(df, ['rr_interval', 'rr', 'RR', 'interval', 'R-R', 'bpm'])\n",
        "    if series is not None: return series, name\n",
        "\n",
        "    # å€¤ã®ç¯„å›²ã§æ¢ã™ (300ms ~ 1500ms)\n",
        "    for col in df.columns:\n",
        "        try:\n",
        "            val = pd.to_numeric(df[col], errors='coerce').dropna()\n",
        "            if len(val) < 10: continue\n",
        "            mean_val = val.mean()\n",
        "            if 300 < mean_val < 1500: return val, f\"{col} (Auto ms)\"\n",
        "            if 0.3 < mean_val < 1.5: return val * 1000, f\"{col} (Auto sec)\"\n",
        "        except: continue\n",
        "    return None, None\n",
        "\n",
        "def calculate_lf_from_rr_high_res(rr_data_ms):\n",
        "    \"\"\"\n",
        "    RRé–“éš”ã‹ã‚‰LFã‚’è¨ˆç®— (é«˜è§£åƒåº¦ãƒ»ã‚®ã‚¶ã‚®ã‚¶é‡è¦–)\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # å¤–ã‚Œå€¤é™¤å»\n",
        "        rr_data = rr_data_ms[(rr_data_ms > 300) & (rr_data_ms < 2000)]\n",
        "        if len(rr_data) < 10: return None\n",
        "\n",
        "        # æ™‚é–“è»¸ä½œæˆ\n",
        "        t_cum = np.cumsum(rr_data) / 1000.0\n",
        "        t_cum = t_cum - t_cum.iloc[0]\n",
        "\n",
        "        # ç­‰é–“éš”ãƒªã‚µãƒ³ãƒ—ãƒªãƒ³ã‚° (4Hz)\n",
        "        fs_interp = 4.0\n",
        "        t_interp = np.arange(0, t_cum.iloc[-1], 1/fs_interp)\n",
        "\n",
        "        # è£œé–“ (Linearã«ã™ã‚‹ã“ã¨ã§ã€ã‚«ãƒ¼ãƒ–ã‚’ä¸¸ã‚ãšå°–ã‚‰ã›ã‚‹)\n",
        "        f_int = interp1d(t_cum, rr_data, kind='linear', fill_value=\"extrapolate\")\n",
        "        rr_interp = f_int(t_interp)\n",
        "\n",
        "        # --- ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿è¨­å®š (ã‚®ã‚¶ã‚®ã‚¶é‡è¦–) ---\n",
        "        window_sec = 60 # çª“å¹… (è¨ˆç®—ã«å¿…è¦ãªæœ€ä½é™ã®é•·ã•)\n",
        "        step_sec = 1    # â˜…ã“ã“é‡è¦: 1ç§’ã”ã¨ã«è¨ˆç®— (å‰å›ã¯5~10ç§’) -> åœ§å€’çš„ã«ç´°ã‹ããªã‚‹\n",
        "\n",
        "        nperseg = int(window_sec * fs_interp)\n",
        "        step_samples = int(step_sec * fs_interp)\n",
        "\n",
        "        if len(rr_interp) < nperseg: return None\n",
        "\n",
        "        lf_vals = []\n",
        "        t_vals = []\n",
        "\n",
        "        # ã‚¹ãƒ©ã‚¤ãƒ‡ã‚£ãƒ³ã‚°ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦è¨ˆç®—\n",
        "        for i in range(0, len(rr_interp) - nperseg, step_samples):\n",
        "            segment = rr_interp[i : i + nperseg]\n",
        "\n",
        "            # ã‚¦ã‚§ãƒ«ãƒæ³• (nfftã‚’å°ã•ãã—ã™ããªã„)\n",
        "            freqs, psd = signal.welch(segment, fs=fs_interp, nperseg=nperseg)\n",
        "\n",
        "            # LFå¸¯åŸŸç©åˆ†\n",
        "            lf_pow = np.trapz(psd[(freqs >= 0.04) & (freqs <= 0.15)], freqs[(freqs >= 0.04) & (freqs <= 0.15)])\n",
        "\n",
        "            lf_vals.append(lf_pow)\n",
        "            t_vals.append(t_interp[i + nperseg//2]) # çª“ã®ä¸­å¤®æ™‚åˆ»\n",
        "\n",
        "        # å…ƒã®æ™‚é–“è»¸ã«åˆã‚ã›ã¦å¼•ãä¼¸ã°ã— (ç·šå½¢è£œé–“ã§ã‚®ã‚¶ã‚®ã‚¶ç¶­æŒ)\n",
        "        x_target = np.linspace(0, t_cum.iloc[-1], len(rr_data_ms))\n",
        "        if len(t_vals) > 1:\n",
        "            f_res = interp1d(t_vals, lf_vals, kind='linear', fill_value=\"extrapolate\")\n",
        "            return pd.Series(f_res(x_target), index=rr_data_ms.index)\n",
        "        return None\n",
        "\n",
        "    except Exception as e:\n",
        "        return None\n",
        "\n",
        "def min_max_norm(series):\n",
        "    if len(series) == 0: return series\n",
        "    scaler = MinMaxScaler()\n",
        "    return scaler.fit_transform(series.values.reshape(-1, 1)).flatten()\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# 3. ãƒ¡ã‚¤ãƒ³å‡¦ç†\n",
        "# ---------------------------------------------------------\n",
        "print(f\"ğŸš€ è§£æé–‹å§‹: {len(subject_list)*len(phase_list)} ãƒ‘ã‚¿ãƒ¼ãƒ³ (Smoothing: OFF)\\n\")\n",
        "\n",
        "success_count = 0\n",
        "\n",
        "for subject in subject_list:\n",
        "    for phase in phase_list:\n",
        "        print(f\"[{subject} - {phase}]\", end=\" \")\n",
        "\n",
        "        # ãƒ•ã‚¡ã‚¤ãƒ«æ¤œç´¢\n",
        "        path_eeg = find_file_fuzzy(base_search_path, subject, phase, ['PLI', 'pli'])\n",
        "        path_rr = find_file_fuzzy(base_search_path, subject, phase, ['rr_interval', 'RR', 'rr', 'interval'])\n",
        "\n",
        "        if not path_eeg or not path_rr:\n",
        "            print(\"-> âš ï¸ Skip (File Missing)\")\n",
        "            continue\n",
        "\n",
        "        try:\n",
        "            # èª­ã¿è¾¼ã¿\n",
        "            df_eeg = pd.read_csv(path_eeg)\n",
        "            df_rr = pd.read_csv(path_rr)\n",
        "\n",
        "            # ãƒ‡ãƒ¼ã‚¿å–å¾—\n",
        "            eeg_data, eeg_col = get_column_data(df_eeg, ['FCz', 'fcz'])\n",
        "            rr_vals, rr_name = smart_extract_rr_data(df_rr)\n",
        "\n",
        "            if eeg_data is None or rr_vals is None:\n",
        "                print(\"-> âš ï¸ Skip (Column Missing)\")\n",
        "                continue\n",
        "\n",
        "            eeg_data = eeg_data.fillna(method='ffill').fillna(method='bfill')\n",
        "\n",
        "            # LFè¨ˆç®— (High Res)\n",
        "            lf_data = calculate_lf_from_rr_high_res(rr_vals)\n",
        "\n",
        "            if lf_data is None:\n",
        "                print(\"-> âŒ LF Calc Failed (Too short?)\")\n",
        "                continue\n",
        "\n",
        "            # --- æç”» (å¹³å‡åŒ–ãªã—ï¼) ---\n",
        "            min_len = min(len(eeg_data), len(lf_data))\n",
        "\n",
        "            # â˜…ãƒã‚¤ãƒ³ãƒˆ: rolling().mean() ã‚’ä½¿ã„ã¾ã›ã‚“ã€‚ãã®ã¾ã¾å‡ºã—ã¾ã™ã€‚\n",
        "            eeg_vis = eeg_data.iloc[:min_len]\n",
        "            lf_vis = lf_data.iloc[:min_len]\n",
        "\n",
        "            fig, ax1 = plt.subplots(figsize=(12, 5))\n",
        "\n",
        "            # EEG: ç”Ÿãƒ‡ãƒ¼ã‚¿ãªã®ã§ã‹ãªã‚Šå¤ªããªã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ãŒã€è¦æ±‚é€šã‚Šãã®ã¾ã¾å‡ºã—ã¾ã™\n",
        "            # alpha=0.6 ã§å°‘ã—é€ã‘ã•ã›ã¦ã€é‡ãªã‚Šã‚’è¦‹ã‚„ã™ãã—ã¾ã™\n",
        "            ax1.plot(min_max_norm(eeg_vis), color='blue', label='EEG (FCz)', alpha=0.6, linewidth=0.8)\n",
        "            ax1.set_ylabel('EEG Power', color='blue')\n",
        "\n",
        "            # LF: 1ç§’åˆ»ã¿è¨ˆç®—çµæœã‚’ãã®ã¾ã¾è¡¨ç¤º (ã‚ªãƒ¬ãƒ³ã‚¸)\n",
        "            ax2 = ax1.twinx()\n",
        "            ax2.plot(min_max_norm(lf_vis), color='orange', label='HRV (LF)', alpha=0.9, linewidth=1.2)\n",
        "            ax2.set_ylabel('LF Power', color='orange')\n",
        "\n",
        "            plt.title(f\"{subject} - {phase}\")\n",
        "\n",
        "            # å‡¡ä¾‹\n",
        "            lines1, labels1 = ax1.get_legend_handles_labels()\n",
        "            lines2, labels2 = ax2.get_legend_handles_labels()\n",
        "            ax1.legend(lines1 + lines2, labels1 + labels2, loc='upper left')\n",
        "\n",
        "            # ä¿å­˜\n",
        "            out_path = os.path.join(output_dir, f\"{subject}_{phase}.png\")\n",
        "            plt.savefig(out_path)\n",
        "            plt.close()\n",
        "\n",
        "            print(\"âœ… OK\")\n",
        "            success_count += 1\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"-> âŒ Error: {e}\")\n",
        "\n",
        "# ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å‡¦ç†\n",
        "if success_count > 0:\n",
        "    print(f\"\\nğŸ‰ {success_count} ä»¶ã®ã‚°ãƒ©ãƒ•ã‚’ä½œæˆã—ã¾ã—ãŸã€‚åœ§ç¸®ã—ã¦ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ã¾ã™...\")\n",
        "    shutil.make_archive('Final_Results_Raw', 'zip', output_dir)\n",
        "    from google.colab import files\n",
        "    files.download('Final_Results_Raw.zip')\n",
        "else:\n",
        "    print(\"\\nâš ï¸ ç”»åƒãŒ1æšã‚‚ä½œæˆã•ã‚Œã¾ã›ã‚“ã§ã—ãŸã€‚\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 935
        },
        "id": "O-qEtdvLWSqz",
        "outputId": "5f55b869-9d89-43fa-908c-0e5f4b79b7bb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸš€ è§£æé–‹å§‹: 50 ãƒ‘ã‚¿ãƒ¼ãƒ³ (Smoothing: OFF)\n",
            "\n",
            "[ishida - rest] âœ… OK\n",
            "[ishida - boredom] âœ… OK\n",
            "[ishida - flow] âœ… OK\n",
            "[ishida - flow_ultra] âœ… OK\n",
            "[ishida - overload] âœ… OK\n",
            "[yamamoto - rest] âœ… OK\n",
            "[yamamoto - boredom] âœ… OK\n",
            "[yamamoto - flow] âœ… OK\n",
            "[yamamoto - flow_ultra] âœ… OK\n",
            "[yamamoto - overload] âœ… OK\n",
            "[ohashi - rest] âœ… OK\n",
            "[ohashi - boredom] âœ… OK\n",
            "[ohashi - flow] âœ… OK\n",
            "[ohashi - flow_ultra] âœ… OK\n",
            "[ohashi - overload] âœ… OK\n",
            "[miyake - rest] âœ… OK\n",
            "[miyake - boredom] âœ… OK\n",
            "[miyake - flow] âœ… OK\n",
            "[miyake - flow_ultra] âœ… OK\n",
            "[miyake - overload] âœ… OK\n",
            "[mitsuhashi - rest] âœ… OK\n",
            "[mitsuhashi - boredom] âœ… OK\n",
            "[mitsuhashi - flow] âœ… OK\n",
            "[mitsuhashi - flow_ultra] âœ… OK\n",
            "[mitsuhashi - overload] âœ… OK\n",
            "[ko - rest] âœ… OK\n",
            "[ko - boredom] âœ… OK\n",
            "[ko - flow] âœ… OK\n",
            "[ko - flow_ultra] âœ… OK\n",
            "[ko - overload] âœ… OK\n",
            "[hayato - rest] âœ… OK\n",
            "[hayato - boredom] âœ… OK\n",
            "[hayato - flow] âœ… OK\n",
            "[hayato - flow_ultra] âœ… OK\n",
            "[hayato - overload] âœ… OK\n",
            "[chika - rest] âœ… OK\n",
            "[chika - boredom] âœ… OK\n",
            "[chika - flow] âœ… OK\n",
            "[chika - flow_ultra] âœ… OK\n",
            "[chika - overload] âœ… OK\n",
            "[nagomi - rest] âœ… OK\n",
            "[nagomi - boredom] âœ… OK\n",
            "[nagomi - flow] âœ… OK\n",
            "[nagomi - flow_ultra] âœ… OK\n",
            "[nagomi - overload] âœ… OK\n",
            "[mio - rest] âœ… OK\n",
            "[mio - boredom] âœ… OK\n",
            "[mio - flow] âœ… OK\n",
            "[mio - flow_ultra] âœ… OK\n",
            "[mio - overload] âœ… OK\n",
            "\n",
            "ğŸ‰ 50 ä»¶ã®ã‚°ãƒ©ãƒ•ã‚’ä½œæˆã—ã¾ã—ãŸã€‚åœ§ç¸®ã—ã¦ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ã¾ã™...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_f59cce2f-5674-4890-8aba-7bde55b0f6d5\", \"Final_Results_Raw.zip\", 2632154)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# ã€è¶…é«˜æ„Ÿåº¦ç‰ˆã€‘EEG & HRV (Sensitive LF + Raw RR)\n",
        "# =============================================================================\n",
        "# å¤‰æ›´ç‚¹:\n",
        "# 1. è¨ˆç®—çª“(Window)ã‚’60ç§’ -> 25ç§’ã«çŸ­ç¸® (ã“ã‚Œä»¥ä¸ŠçŸ­ã„ã¨LFã®å®šç¾©0.04HzãŒè¨ˆç®—ä¸å¯ã«ãªã‚Šã¾ã™)\n",
        "# 2. èƒŒæ™¯ã«ã€Œå…ƒã®RRé–“éš”(Raw)ã€ã‚’è–„ãè¡¨ç¤ºã—ã€æœ¬æ¥ã®ã‚®ã‚¶ã‚®ã‚¶ã‚’ç¢ºèªã§ãã‚‹ã‚ˆã†ã«ã—ã¾ã™ã€‚\n",
        "# =============================================================================\n",
        "\n",
        "import sys\n",
        "import os\n",
        "import glob\n",
        "import shutil\n",
        "import warnings\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy import signal\n",
        "from scipy.interpolate import interp1d\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from google.colab import drive\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# --- è¨­å®š ---\n",
        "if not os.path.exists('/content/drive'):\n",
        "    drive.mount('/content/drive')\n",
        "\n",
        "base_search_path = \"/content/drive/MyDrive/csv\"\n",
        "\n",
        "subject_list = [\n",
        "    'ishida', 'yamamoto', 'ohashi', 'miyake', 'mitsuhashi',\n",
        "    'ko', 'hayato', 'chika', 'nagomi', 'mio'\n",
        "]\n",
        "\n",
        "phase_list = ['rest', 'boredom', 'flow', 'flow_ultra', 'overload']\n",
        "\n",
        "output_dir = \"Graphs_Sensitive_LF\"\n",
        "if os.path.exists(output_dir):\n",
        "    shutil.rmtree(output_dir)\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "\n",
        "# --- é–¢æ•°ç¾¤ ---\n",
        "def find_file_fuzzy(base_path, subject, phase, keywords):\n",
        "    search_pattern = os.path.join(base_path, \"**\", f\"*{subject}*{phase}*.csv\")\n",
        "    candidates = glob.glob(search_pattern, recursive=True)\n",
        "    for path in candidates:\n",
        "        if any(k.lower() in os.path.basename(path).lower() for k in keywords):\n",
        "            return path\n",
        "    return None\n",
        "\n",
        "def get_column_data(df, target_names):\n",
        "    df.columns = [str(c).strip() for c in df.columns]\n",
        "    for col in df.columns:\n",
        "        for target in target_names:\n",
        "            if col.lower() == target.lower():\n",
        "                return df[col], col\n",
        "    return None, None\n",
        "\n",
        "def smart_extract_rr_data(df):\n",
        "    series, name = get_column_data(df, ['rr_interval', 'rr', 'RR', 'interval', 'R-R', 'bpm'])\n",
        "    if series is not None: return series, name\n",
        "    for col in df.columns:\n",
        "        try:\n",
        "            val = pd.to_numeric(df[col], errors='coerce').dropna()\n",
        "            if len(val) < 10: continue\n",
        "            mean_val = val.mean()\n",
        "            if 300 < mean_val < 1500: return val, f\"{col} (Auto ms)\"\n",
        "            if 0.3 < mean_val < 1.5: return val * 1000, f\"{col} (Auto sec)\"\n",
        "        except: continue\n",
        "    return None, None\n",
        "\n",
        "def calculate_sensitive_lf(rr_data_ms):\n",
        "    \"\"\"\n",
        "    çª“å¹…ã‚’æ¥µé™ã¾ã§çŸ­ãã—ã¦ã€å¤‰å‹•ã‚’æ‹¾ã†LFè¨ˆç®—\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # å¤–ã‚Œå€¤é™¤å»\n",
        "        rr_data = rr_data_ms[(rr_data_ms > 300) & (rr_data_ms < 2000)]\n",
        "        if len(rr_data) < 10: return None, None\n",
        "\n",
        "        # æ™‚é–“è»¸ä½œæˆ\n",
        "        t_cum = np.cumsum(rr_data) / 1000.0\n",
        "        t_cum = t_cum - t_cum.iloc[0]\n",
        "\n",
        "        # ãƒªã‚µãƒ³ãƒ—ãƒªãƒ³ã‚° (4Hz)\n",
        "        fs_interp = 4.0\n",
        "        t_interp = np.arange(0, t_cum.iloc[-1], 1/fs_interp)\n",
        "        f_int = interp1d(t_cum, rr_data, kind='linear', fill_value=\"extrapolate\")\n",
        "        rr_interp = f_int(t_interp)\n",
        "\n",
        "        # â˜…ã“ã“ãŒå¤‰æ›´ç‚¹: Windowã‚’çŸ­ãã™ã‚‹\n",
        "        window_sec = 25 # 25ç§’ (LFã®ä¸‹é™0.04Hz=25ç§’å‘¨æœŸ ãªã®ã§ã€ã“ã‚Œä»¥ä¸ŠçŸ­ãã™ã‚‹ã¨æ•°å­¦çš„ã«è¨ˆç®—ä¸èƒ½)\n",
        "        step_sec = 1    # 1ç§’åˆ»ã¿\n",
        "\n",
        "        nperseg = int(window_sec * fs_interp)\n",
        "        step_samples = int(step_sec * fs_interp)\n",
        "\n",
        "        if len(rr_interp) < nperseg: return None, None\n",
        "\n",
        "        lf_vals = []\n",
        "        t_vals = []\n",
        "\n",
        "        for i in range(0, len(rr_interp) - nperseg, step_samples):\n",
        "            segment = rr_interp[i : i + nperseg]\n",
        "            freqs, psd = signal.welch(segment, fs=fs_interp, nperseg=nperseg)\n",
        "            # LFå¸¯åŸŸç©åˆ†\n",
        "            lf_pow = np.trapz(psd[(freqs >= 0.04) & (freqs <= 0.15)], freqs[(freqs >= 0.04) & (freqs <= 0.15)])\n",
        "            lf_vals.append(lf_pow)\n",
        "            t_vals.append(t_interp[i + nperseg//2])\n",
        "\n",
        "        # è£œé–“\n",
        "        x_target = np.linspace(0, t_cum.iloc[-1], len(rr_data_ms))\n",
        "\n",
        "        # LFã®æ™‚ç³»åˆ—\n",
        "        lf_series = None\n",
        "        if len(t_vals) > 1:\n",
        "            f_res = interp1d(t_vals, lf_vals, kind='linear', fill_value=\"extrapolate\")\n",
        "            lf_series = pd.Series(f_res(x_target), index=rr_data_ms.index)\n",
        "\n",
        "        # ç”Ÿã®RRæ™‚ç³»åˆ— (ã‚°ãƒ©ãƒ•èƒŒæ™¯ç”¨)\n",
        "        rr_raw_resampled = pd.Series(interp1d(t_interp, rr_interp, kind='linear')(np.linspace(0, t_interp[-1], len(rr_data_ms))), index=rr_data_ms.index)\n",
        "\n",
        "        return lf_series, rr_raw_resampled\n",
        "\n",
        "    except Exception:\n",
        "        return None, None\n",
        "\n",
        "def min_max_norm(series):\n",
        "    if len(series) == 0: return series\n",
        "    scaler = MinMaxScaler()\n",
        "    return scaler.fit_transform(series.values.reshape(-1, 1)).flatten()\n",
        "\n",
        "\n",
        "# --- ãƒ¡ã‚¤ãƒ³å‡¦ç† ---\n",
        "print(f\"ğŸš€ è§£æé–‹å§‹ (é«˜æ„Ÿåº¦LFãƒ¢ãƒ¼ãƒ‰)\\n\")\n",
        "\n",
        "success_count = 0\n",
        "\n",
        "for subject in subject_list:\n",
        "    for phase in phase_list:\n",
        "        print(f\"[{subject} - {phase}]\", end=\" \")\n",
        "\n",
        "        path_eeg = find_file_fuzzy(base_search_path, subject, phase, ['PLI', 'pli'])\n",
        "        path_rr = find_file_fuzzy(base_search_path, subject, phase, ['rr_interval', 'RR', 'rr', 'interval'])\n",
        "\n",
        "        if not path_eeg or not path_rr:\n",
        "            print(\"-> Skip (Missing)\")\n",
        "            continue\n",
        "\n",
        "        try:\n",
        "            df_eeg = pd.read_csv(path_eeg)\n",
        "            df_rr = pd.read_csv(path_rr)\n",
        "\n",
        "            eeg_data, eeg_col = get_column_data(df_eeg, ['FCz', 'fcz'])\n",
        "            rr_vals, rr_name = smart_extract_rr_data(df_rr)\n",
        "\n",
        "            if eeg_data is None or rr_vals is None:\n",
        "                print(\"-> Skip (No Data)\")\n",
        "                continue\n",
        "\n",
        "            eeg_data = eeg_data.fillna(method='ffill').fillna(method='bfill')\n",
        "\n",
        "            # LFè¨ˆç®— (Sensitive)\n",
        "            lf_data, rr_raw_data = calculate_sensitive_lf(rr_vals)\n",
        "\n",
        "            if lf_data is None:\n",
        "                print(\"-> LF Calc Failed\")\n",
        "                continue\n",
        "\n",
        "            # æç”»\n",
        "            min_len = min(len(eeg_data), len(lf_data))\n",
        "            eeg_vis = eeg_data.iloc[:min_len]\n",
        "            lf_vis = lf_data.iloc[:min_len]\n",
        "            rr_vis = rr_raw_data.iloc[:min_len] # èƒŒæ™¯ç”¨\n",
        "\n",
        "            fig, ax1 = plt.subplots(figsize=(12, 5))\n",
        "\n",
        "            # 1. èƒŒæ™¯ã«è–„ãã€Œç”Ÿã®RRé–“éš”(Raw)ã€ã‚’æç”» -> ã“ã‚ŒãŒä¸€ç•ªã‚®ã‚¶ã‚®ã‚¶\n",
        "            ax1.plot(min_max_norm(rr_vis), color='gray', alpha=0.15, label='Raw RR Interval')\n",
        "\n",
        "            # 2. EEG (é’)\n",
        "            ax1.plot(min_max_norm(eeg_vis), color='blue', label='EEG (FCz)', alpha=0.8, linewidth=1.0)\n",
        "            ax1.set_ylabel('EEG Power', color='blue')\n",
        "\n",
        "            # 3. LF (ã‚ªãƒ¬ãƒ³ã‚¸) - çª“ã‚’çŸ­ãã—ãŸã®ã§ä»¥å‰ã‚ˆã‚Šã‚®ã‚¶ã‚®ã‚¶ãªã¯ãš\n",
        "            ax2 = ax1.twinx()\n",
        "            ax2.plot(min_max_norm(lf_vis), color='orange', label='HRV (Sensitive LF)', alpha=0.9, linewidth=1.5)\n",
        "            ax2.set_ylabel('LF Power', color='orange')\n",
        "\n",
        "            plt.title(f\"{subject} - {phase}\")\n",
        "\n",
        "            # å‡¡ä¾‹æ•´ç†\n",
        "            lines1, labels1 = ax1.get_legend_handles_labels()\n",
        "            lines2, labels2 = ax2.get_legend_handles_labels()\n",
        "            ax1.legend(lines1 + lines2, labels1 + labels2, loc='upper left')\n",
        "\n",
        "            plt.savefig(os.path.join(output_dir, f\"{subject}_{phase}.png\"))\n",
        "            plt.close()\n",
        "\n",
        "            print(\"âœ… OK\")\n",
        "            success_count += 1\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"-> Error: {e}\")\n",
        "\n",
        "if success_count > 0:\n",
        "    shutil.make_archive('Final_Results_Sensitive', 'zip', output_dir)\n",
        "    from google.colab import files\n",
        "    files.download('Final_Results_Sensitive.zip')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 901
        },
        "id": "_u-0aJJvXEck",
        "outputId": "bd12f52b-5cf1-4aa2-fe30-0e8bb4d9b41b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸš€ è§£æé–‹å§‹ (é«˜æ„Ÿåº¦LFãƒ¢ãƒ¼ãƒ‰)\n",
            "\n",
            "[ishida - rest] âœ… OK\n",
            "[ishida - boredom] âœ… OK\n",
            "[ishida - flow] âœ… OK\n",
            "[ishida - flow_ultra] âœ… OK\n",
            "[ishida - overload] âœ… OK\n",
            "[yamamoto - rest] âœ… OK\n",
            "[yamamoto - boredom] âœ… OK\n",
            "[yamamoto - flow] âœ… OK\n",
            "[yamamoto - flow_ultra] âœ… OK\n",
            "[yamamoto - overload] âœ… OK\n",
            "[ohashi - rest] âœ… OK\n",
            "[ohashi - boredom] âœ… OK\n",
            "[ohashi - flow] âœ… OK\n",
            "[ohashi - flow_ultra] âœ… OK\n",
            "[ohashi - overload] âœ… OK\n",
            "[miyake - rest] âœ… OK\n",
            "[miyake - boredom] âœ… OK\n",
            "[miyake - flow] âœ… OK\n",
            "[miyake - flow_ultra] âœ… OK\n",
            "[miyake - overload] âœ… OK\n",
            "[mitsuhashi - rest] âœ… OK\n",
            "[mitsuhashi - boredom] âœ… OK\n",
            "[mitsuhashi - flow] âœ… OK\n",
            "[mitsuhashi - flow_ultra] âœ… OK\n",
            "[mitsuhashi - overload] âœ… OK\n",
            "[ko - rest] âœ… OK\n",
            "[ko - boredom] âœ… OK\n",
            "[ko - flow] âœ… OK\n",
            "[ko - flow_ultra] âœ… OK\n",
            "[ko - overload] âœ… OK\n",
            "[hayato - rest] âœ… OK\n",
            "[hayato - boredom] âœ… OK\n",
            "[hayato - flow] âœ… OK\n",
            "[hayato - flow_ultra] âœ… OK\n",
            "[hayato - overload] âœ… OK\n",
            "[chika - rest] âœ… OK\n",
            "[chika - boredom] âœ… OK\n",
            "[chika - flow] âœ… OK\n",
            "[chika - flow_ultra] âœ… OK\n",
            "[chika - overload] âœ… OK\n",
            "[nagomi - rest] âœ… OK\n",
            "[nagomi - boredom] âœ… OK\n",
            "[nagomi - flow] âœ… OK\n",
            "[nagomi - flow_ultra] âœ… OK\n",
            "[nagomi - overload] âœ… OK\n",
            "[mio - rest] âœ… OK\n",
            "[mio - boredom] âœ… OK\n",
            "[mio - flow] âœ… OK\n",
            "[mio - flow_ultra] âœ… OK\n",
            "[mio - overload] âœ… OK\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_3b26777f-277b-4774-86c4-7366a9893c5e\", \"Final_Results_Sensitive.zip\", 4273788)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# Project: Flow State EEG & HRV Analysis\n",
        "# Mode: Raw Sensitivity (No Smoothing, High-Resolution LF)\n",
        "# =============================================================================\n",
        "\n",
        "import sys\n",
        "import os\n",
        "import glob\n",
        "import shutil\n",
        "import warnings\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy import signal\n",
        "from scipy.interpolate import interp1d\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from google.colab import drive\n",
        "\n",
        "# è­¦å‘Šãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’éè¡¨ç¤º\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# 1. è¨­å®šã‚¨ãƒªã‚¢\n",
        "# ---------------------------------------------------------\n",
        "\n",
        "# Google Drive ãƒã‚¦ãƒ³ãƒˆ\n",
        "if not os.path.exists('/content/drive'):\n",
        "    drive.mount('/content/drive')\n",
        "\n",
        "# ãƒ‡ãƒ¼ã‚¿æ ¼ç´è¦ªãƒ•ã‚©ãƒ«ãƒ€\n",
        "base_search_path = \"/content/drive/MyDrive/csv\"\n",
        "\n",
        "# åˆ†æå¯¾è±¡ãƒªã‚¹ãƒˆ\n",
        "subject_list = [\n",
        "    'ishida', 'yamamoto', 'ohashi', 'miyake', 'mitsuhashi',\n",
        "    'ko', 'hayato', 'chika', 'nagomi', 'mio'\n",
        "]\n",
        "\n",
        "# åˆ†æãƒ•ã‚§ãƒ¼ã‚ºãƒªã‚¹ãƒˆ\n",
        "phase_list = ['rest', 'boredom', 'flow', 'flow_ultra', 'overload']\n",
        "\n",
        "# å‡ºåŠ›ãƒ•ã‚©ãƒ«ãƒ€å\n",
        "output_dir = \"Flow_Analysis_Results_Raw\"\n",
        "\n",
        "# ä»¥å‰ã®çµæœãŒã‚ã‚Œã°å‰Šé™¤ã—ã¦ä½œã‚Šç›´ã™\n",
        "if os.path.exists(output_dir):\n",
        "    shutil.rmtree(output_dir)\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# 2. é–¢æ•°å®šç¾©\n",
        "# ---------------------------------------------------------\n",
        "\n",
        "def find_file_fuzzy(base_path, subject, phase, keywords):\n",
        "    \"\"\"\n",
        "    æŒ‡å®šã•ã‚ŒãŸã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’å«ã‚€ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ•ã‚©ãƒ«ãƒ€éšå±¤ä¸‹ã‹ã‚‰æ¤œç´¢\n",
        "    \"\"\"\n",
        "    search_pattern = os.path.join(base_path, \"**\", f\"*{subject}*{phase}*.csv\")\n",
        "    candidates = glob.glob(search_pattern, recursive=True)\n",
        "    for path in candidates:\n",
        "        # ãƒ•ã‚¡ã‚¤ãƒ«åã«ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãŒå«ã¾ã‚Œã‚‹ã‹ãƒã‚§ãƒƒã‚¯\n",
        "        if any(k.lower() in os.path.basename(path).lower() for k in keywords):\n",
        "            return path\n",
        "    return None\n",
        "\n",
        "def get_column_data(df, target_names):\n",
        "    \"\"\"\n",
        "    ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã‹ã‚‰å¯¾è±¡ã®ã‚«ãƒ©ãƒ ã‚’æŸ”è»Ÿã«å–å¾—\n",
        "    \"\"\"\n",
        "    df.columns = [str(c).strip() for c in df.columns]\n",
        "    for col in df.columns:\n",
        "        for target in target_names:\n",
        "            if col.lower() == target.lower():\n",
        "                return df[col], col\n",
        "    return None, None\n",
        "\n",
        "def smart_extract_rr_data(df):\n",
        "    \"\"\"\n",
        "    RRé–“éš”ãƒ‡ãƒ¼ã‚¿ã‚’è‡ªå‹•ç‰¹å®šï¼ˆåˆ—åã¾ãŸã¯æ•°å€¤ç¯„å›²ã‹ã‚‰åˆ¤æ–­ï¼‰\n",
        "    \"\"\"\n",
        "    # 1. åˆ—åã§æ¤œç´¢\n",
        "    series, name = get_column_data(df, ['rr_interval', 'rr', 'RR', 'interval', 'R-R', 'bpm'])\n",
        "    if series is not None: return series, name\n",
        "\n",
        "    # 2. æ•°å€¤ç¯„å›²ã§æ¤œç´¢ (300ms ~ 1500ms)\n",
        "    for col in df.columns:\n",
        "        try:\n",
        "            val = pd.to_numeric(df[col], errors='coerce').dropna()\n",
        "            if len(val) < 10: continue\n",
        "            mean_val = val.mean()\n",
        "            # mså˜ä½ã®å ´åˆ\n",
        "            if 300 < mean_val < 1500: return val, f\"{col} (Auto-detect ms)\"\n",
        "            # ç§’å˜ä½ã®å ´åˆ (1000å€ã™ã‚‹)\n",
        "            if 0.3 < mean_val < 1.5: return val * 1000, f\"{col} (Auto-detect sec)\"\n",
        "        except: continue\n",
        "    return None, None\n",
        "\n",
        "def calculate_lf_high_res(rr_data_ms):\n",
        "    \"\"\"\n",
        "    RRé–“éš”ã‹ã‚‰LFæˆåˆ†ã‚’è¨ˆç®—ï¼ˆé«˜è§£åƒåº¦è¨­å®šï¼‰\n",
        "    Window: 60s, Step: 1s\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # å¤–ã‚Œå€¤é™¤å» (300ms ~ 2000msä»¥å¤–ã‚’é™¤å¤–)\n",
        "        rr_data = rr_data_ms[(rr_data_ms > 300) & (rr_data_ms < 2000)]\n",
        "        if len(rr_data) < 10: return None\n",
        "\n",
        "        # æ™‚é–“è»¸ã®ä½œæˆã¨ãƒªã‚µãƒ³ãƒ—ãƒªãƒ³ã‚° (4Hz)\n",
        "        t_cum = np.cumsum(rr_data) / 1000.0\n",
        "        t_cum = t_cum - t_cum.iloc[0]\n",
        "\n",
        "        fs_interp = 4.0\n",
        "        t_interp = np.arange(0, t_cum.iloc[-1], 1/fs_interp)\n",
        "\n",
        "        # ç·šå½¢è£œé–“ (Linear) ã§å¤‰å‹•ã®é‹­ã•ã‚’ç¶­æŒ\n",
        "        f_int = interp1d(t_cum, rr_data, kind='linear', fill_value=\"extrapolate\")\n",
        "        rr_interp = f_int(t_interp)\n",
        "\n",
        "        # ã‚¹ãƒ©ã‚¤ãƒ‡ã‚£ãƒ³ã‚°ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦è¨­å®š\n",
        "        window_sec = 60 # LFè¨ˆç®—ã«å¿…è¦ãªæœ€ä½å¹…\n",
        "        step_sec = 1    # 1ç§’ã”ã¨ã«è¨ˆç®—ï¼ˆé«˜è§£åƒåº¦ï¼‰\n",
        "\n",
        "        nperseg = int(window_sec * fs_interp)\n",
        "        step_samples = int(step_sec * fs_interp)\n",
        "\n",
        "        if len(rr_interp) < nperseg: return None\n",
        "\n",
        "        lf_vals = []\n",
        "        t_vals = []\n",
        "\n",
        "        for i in range(0, len(rr_interp) - nperseg, step_samples):\n",
        "            segment = rr_interp[i : i + nperseg]\n",
        "\n",
        "            # ã‚¦ã‚§ãƒ«ãƒæ³•ã§PSDè¨ˆç®—\n",
        "            freqs, psd = signal.welch(segment, fs=fs_interp, nperseg=nperseg)\n",
        "\n",
        "            # LFå¸¯åŸŸ (0.04-0.15Hz) ã®ãƒ‘ãƒ¯ãƒ¼ç©åˆ†\n",
        "            lf_pow = np.trapz(psd[(freqs >= 0.04) & (freqs <= 0.15)], freqs[(freqs >= 0.04) & (freqs <= 0.15)])\n",
        "\n",
        "            lf_vals.append(lf_pow)\n",
        "            t_vals.append(t_interp[i + nperseg//2])\n",
        "\n",
        "        # å…ƒã®æ™‚é–“è»¸ã«åˆã‚ã›ã¦è£œé–“\n",
        "        x_target = np.linspace(0, t_cum.iloc[-1], len(rr_data_ms))\n",
        "        if len(t_vals) > 1:\n",
        "            f_res = interp1d(t_vals, lf_vals, kind='linear', fill_value=\"extrapolate\")\n",
        "            return pd.Series(f_res(x_target), index=rr_data_ms.index)\n",
        "        return None\n",
        "\n",
        "    except Exception:\n",
        "        return None\n",
        "\n",
        "def min_max_norm(series):\n",
        "    \"\"\"0-1æ­£è¦åŒ–\"\"\"\n",
        "    if len(series) == 0: return series\n",
        "    scaler = MinMaxScaler()\n",
        "    return scaler.fit_transform(series.values.reshape(-1, 1)).flatten()\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# 3. ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œãƒ«ãƒ¼ãƒ—\n",
        "# ---------------------------------------------------------\n",
        "print(f\"ğŸš€ è§£æã‚’é–‹å§‹ã—ã¾ã™...\")\n",
        "print(f\"   å¯¾è±¡: {len(subject_list)}å x {len(phase_list)}ãƒ•ã‚§ãƒ¼ã‚º\")\n",
        "print(f\"   ãƒ¢ãƒ¼ãƒ‰: Smoothing OFF / LF Step 1s\\n\")\n",
        "\n",
        "success_count = 0\n",
        "skip_count = 0\n",
        "\n",
        "for subject in subject_list:\n",
        "    for phase in phase_list:\n",
        "        print(f\"[{subject} - {phase}]\", end=\" \")\n",
        "\n",
        "        # ãƒ•ã‚¡ã‚¤ãƒ«æ¤œç´¢\n",
        "        path_eeg = find_file_fuzzy(base_search_path, subject, phase, ['PLI', 'pli'])\n",
        "        path_rr = find_file_fuzzy(base_search_path, subject, phase, ['rr_interval', 'RR', 'rr', 'interval'])\n",
        "\n",
        "        if not path_eeg or not path_rr:\n",
        "            print(\"-> âš ï¸ Skip (File not found)\")\n",
        "            skip_count += 1\n",
        "            continue\n",
        "\n",
        "        try:\n",
        "            # ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿\n",
        "            df_eeg = pd.read_csv(path_eeg)\n",
        "            df_rr = pd.read_csv(path_rr)\n",
        "\n",
        "            # ã‚«ãƒ©ãƒ ç‰¹å®š\n",
        "            eeg_data, eeg_col = get_column_data(df_eeg, ['FCz', 'fcz'])\n",
        "            rr_vals, rr_name = smart_extract_rr_data(df_rr)\n",
        "\n",
        "            if eeg_data is None or rr_vals is None:\n",
        "                print(\"-> âš ï¸ Skip (Column not found)\")\n",
        "                skip_count += 1\n",
        "                continue\n",
        "\n",
        "            # æ¬ æå‡¦ç†\n",
        "            eeg_data = eeg_data.fillna(method='ffill').fillna(method='bfill')\n",
        "\n",
        "            # LFè¨ˆç®—\n",
        "            lf_data = calculate_lf_high_res(rr_vals)\n",
        "\n",
        "            if lf_data is None:\n",
        "                print(\"-> âŒ LF Calc Failed\")\n",
        "                skip_count += 1\n",
        "                continue\n",
        "\n",
        "            # æç”» (ã‚¹ãƒ ãƒ¼ã‚¸ãƒ³ã‚°ãªã—)\n",
        "            min_len = min(len(eeg_data), len(lf_data))\n",
        "            eeg_vis = eeg_data.iloc[:min_len]\n",
        "            lf_vis = lf_data.iloc[:min_len]\n",
        "\n",
        "            fig, ax1 = plt.subplots(figsize=(12, 5))\n",
        "\n",
        "            # EEG (é’)\n",
        "            ax1.plot(min_max_norm(eeg_vis), color='blue', label='EEG (FCz)', alpha=0.6, linewidth=0.8)\n",
        "            ax1.set_ylabel('EEG Power (Normalized)', color='blue')\n",
        "\n",
        "            # HRV (ã‚ªãƒ¬ãƒ³ã‚¸)\n",
        "            ax2 = ax1.twinx()\n",
        "            ax2.plot(min_max_norm(lf_vis), color='orange', label='HRV (LF)', alpha=0.9, linewidth=1.2)\n",
        "            ax2.set_ylabel('LF Power (Normalized)', color='orange')\n",
        "\n",
        "            plt.title(f\"Subject: {subject} | Phase: {phase}\")\n",
        "\n",
        "            # å‡¡ä¾‹\n",
        "            lines1, labels1 = ax1.get_legend_handles_labels()\n",
        "            lines2, labels2 = ax2.get_legend_handles_labels()\n",
        "            ax1.legend(lines1 + lines2, labels1 + labels2, loc='upper left')\n",
        "\n",
        "            # ä¿å­˜\n",
        "            save_path = os.path.join(output_dir, f\"{subject}_{phase}.png\")\n",
        "            plt.savefig(save_path)\n",
        "            plt.close()\n",
        "\n",
        "            print(\"âœ… OK\")\n",
        "            success_count += 1\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"-> âŒ Error: {e}\")\n",
        "            skip_count += 1\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# 4. çµ‚äº†å‡¦ç† (Zipä½œæˆã¨ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰)\n",
        "# ---------------------------------------------------------\n",
        "if success_count > 0:\n",
        "    print(f\"\\nğŸ‰ å®Œäº†ã—ã¾ã—ãŸï¼ (æˆåŠŸ: {success_count}, ã‚¹ã‚­ãƒƒãƒ—/å¤±æ•—: {skip_count})\")\n",
        "    zip_filename = 'final_result_raw'\n",
        "    shutil.make_archive(zip_filename, 'zip', output_dir)\n",
        "    from google.colab import files\n",
        "    files.download(f'{zip_filename}.zip')\n",
        "    print(f\"â¬‡ï¸ '{zip_filename}.zip' ã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ãŒé–‹å§‹ã•ã‚Œã¾ã™ã€‚\")\n",
        "else:\n",
        "    print(\"\\nâš ï¸ ç”»åƒãŒä½œæˆã•ã‚Œã¾ã›ã‚“ã§ã—ãŸã€‚\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 986
        },
        "id": "iIMlo92pX5Dd",
        "outputId": "7abbb449-d878-43c3-e47b-814052d2fc38"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸš€ è§£æã‚’é–‹å§‹ã—ã¾ã™...\n",
            "   å¯¾è±¡: 10å x 5ãƒ•ã‚§ãƒ¼ã‚º\n",
            "   ãƒ¢ãƒ¼ãƒ‰: Smoothing OFF / LF Step 1s\n",
            "\n",
            "[ishida - rest] âœ… OK\n",
            "[ishida - boredom] âœ… OK\n",
            "[ishida - flow] âœ… OK\n",
            "[ishida - flow_ultra] âœ… OK\n",
            "[ishida - overload] âœ… OK\n",
            "[yamamoto - rest] âœ… OK\n",
            "[yamamoto - boredom] âœ… OK\n",
            "[yamamoto - flow] âœ… OK\n",
            "[yamamoto - flow_ultra] âœ… OK\n",
            "[yamamoto - overload] âœ… OK\n",
            "[ohashi - rest] âœ… OK\n",
            "[ohashi - boredom] âœ… OK\n",
            "[ohashi - flow] âœ… OK\n",
            "[ohashi - flow_ultra] âœ… OK\n",
            "[ohashi - overload] âœ… OK\n",
            "[miyake - rest] âœ… OK\n",
            "[miyake - boredom] âœ… OK\n",
            "[miyake - flow] âœ… OK\n",
            "[miyake - flow_ultra] âœ… OK\n",
            "[miyake - overload] âœ… OK\n",
            "[mitsuhashi - rest] âœ… OK\n",
            "[mitsuhashi - boredom] âœ… OK\n",
            "[mitsuhashi - flow] âœ… OK\n",
            "[mitsuhashi - flow_ultra] âœ… OK\n",
            "[mitsuhashi - overload] âœ… OK\n",
            "[ko - rest] âœ… OK\n",
            "[ko - boredom] âœ… OK\n",
            "[ko - flow] âœ… OK\n",
            "[ko - flow_ultra] âœ… OK\n",
            "[ko - overload] âœ… OK\n",
            "[hayato - rest] âœ… OK\n",
            "[hayato - boredom] âœ… OK\n",
            "[hayato - flow] âœ… OK\n",
            "[hayato - flow_ultra] âœ… OK\n",
            "[hayato - overload] âœ… OK\n",
            "[chika - rest] âœ… OK\n",
            "[chika - boredom] âœ… OK\n",
            "[chika - flow] âœ… OK\n",
            "[chika - flow_ultra] âœ… OK\n",
            "[chika - overload] âœ… OK\n",
            "[nagomi - rest] âœ… OK\n",
            "[nagomi - boredom] âœ… OK\n",
            "[nagomi - flow] âœ… OK\n",
            "[nagomi - flow_ultra] âœ… OK\n",
            "[nagomi - overload] âœ… OK\n",
            "[mio - rest] âœ… OK\n",
            "[mio - boredom] âœ… OK\n",
            "[mio - flow] âœ… OK\n",
            "[mio - flow_ultra] âœ… OK\n",
            "[mio - overload] âœ… OK\n",
            "\n",
            "ğŸ‰ å®Œäº†ã—ã¾ã—ãŸï¼ (æˆåŠŸ: 50, ã‚¹ã‚­ãƒƒãƒ—/å¤±æ•—: 0)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_d2791e33-8dae-448e-80d9-cd55b9f5dd2a\", \"final_result_raw.zip\", 2919143)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "â¬‡ï¸ 'final_result_raw.zip' ã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ãŒé–‹å§‹ã•ã‚Œã¾ã™ã€‚\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# ã€ç”»åƒçµåˆã€‘50æšã®ã‚°ãƒ©ãƒ•ã‚’ 10è¡Œx5åˆ— ã®è¡Œåˆ—ç”»åƒã«ã™ã‚‹ã‚³ãƒ¼ãƒ‰\n",
        "# =============================================================================\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "import os\n",
        "\n",
        "# --- è¨­å®š ---\n",
        "\n",
        "# ç”»åƒãŒå…¥ã£ã¦ã„ã‚‹ãƒ•ã‚©ãƒ«ãƒ€ (ç›´å‰ã®ã‚³ãƒ¼ãƒ‰ã®å‡ºåŠ›å…ˆ)\n",
        "input_dir = \"Flow_Analysis_Results_Raw\"\n",
        "\n",
        "# ä¿å­˜ã™ã‚‹ãƒ•ã‚¡ã‚¤ãƒ«å\n",
        "output_filename = \"Combined_Matrix_5x10.png\"\n",
        "\n",
        "# ãƒªã‚¹ãƒˆå®šç¾© (ä¸¦ã³é †ç”¨)\n",
        "subject_list = [\n",
        "    'ishida', 'yamamoto', 'ohashi', 'miyake', 'mitsuhashi',\n",
        "    'ko', 'hayato', 'chika', 'nagomi', 'mio'\n",
        "]\n",
        "\n",
        "phase_list = ['rest', 'boredom', 'flow', 'flow_ultra', 'overload']\n",
        "\n",
        "# --- çµåˆå‡¦ç† ---\n",
        "\n",
        "print(\"ğŸ–¼ï¸ ç”»åƒã‚’çµåˆä¸­...\")\n",
        "\n",
        "# 1. å·¨å¤§ãªã‚­ãƒ£ãƒ³ãƒã‚¹ã‚’ä½œæˆ (æ¨ª25ã‚¤ãƒ³ãƒ x ç¸¦40ã‚¤ãƒ³ãƒ)\n",
        "# â€»ã‚°ãƒ©ãƒ•ã®ç¸¦æ¨ªæ¯”ã«åˆã‚ã›ã¦ figsize ã‚’èª¿æ•´ã—ã¦ãã ã•ã„\n",
        "fig, axes = plt.subplots(nrows=10, ncols=5, figsize=(25, 40))\n",
        "\n",
        "# ä½™ç™½ã®èª¿æ•´ (wspace=æ¨ªã®é–“éš”, hspace=ç¸¦ã®é–“éš”)\n",
        "plt.subplots_adjust(wspace=0.05, hspace=0.1)\n",
        "\n",
        "for i, subject in enumerate(subject_list):\n",
        "    for j, phase in enumerate(phase_list):\n",
        "\n",
        "        ax = axes[i, j]\n",
        "\n",
        "        # èª­ã¿è¾¼ã‚€ãƒ•ã‚¡ã‚¤ãƒ«å\n",
        "        # â€»ã‚‚ã—ãƒ•ã‚¡ã‚¤ãƒ«åç”Ÿæˆãƒ«ãƒ¼ãƒ«ãŒé•ã†å ´åˆã¯ã“ã“ã‚’ä¿®æ­£\n",
        "        filename = f\"{subject}_{phase}.png\"\n",
        "        filepath = os.path.join(input_dir, filename)\n",
        "\n",
        "        # ç”»åƒãŒå­˜åœ¨ã™ã‚‹ã‹ãƒã‚§ãƒƒã‚¯ã—ã¦è¡¨ç¤º\n",
        "        if os.path.exists(filepath):\n",
        "            img = mpimg.imread(filepath)\n",
        "            ax.imshow(img)\n",
        "        else:\n",
        "            # ç”»åƒãŒãªã„å ´åˆã¯ç©ºç™½ã«ã—ã¤ã¤ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸\n",
        "            ax.text(0.5, 0.5, \"No Data\", ha='center', va='center', fontsize=20, color='gray')\n",
        "\n",
        "        # ã‚°ãƒ©ãƒ•è‡ªä½“ã®æ ç·šã‚„ç›®ç››ã‚Šã‚’æ¶ˆã™ (ç”»åƒã®ä¸­ã«æ—¢ã«ã‚ã‚‹ãŸã‚)\n",
        "        ax.axis('off')\n",
        "\n",
        "        # --- ãƒ©ãƒ™ãƒ«ä»˜ã‘ (å’è«–ç”¨ã«è¦‹ã‚„ã™ã) ---\n",
        "\n",
        "        # 1è¡Œç›®ã ã‘ã«ã€Œãƒ•ã‚§ãƒ¼ã‚ºåã€ã‚’ä¸Šéƒ¨ã«è¡¨ç¤º\n",
        "        if i == 0:\n",
        "            ax.set_title(phase.upper(), fontsize=24, weight='bold', pad=20)\n",
        "\n",
        "        # 1åˆ—ç›®ã ã‘ã«ã€Œè¢«é¨“è€…åã€ã‚’å·¦å´ã«è¡¨ç¤º\n",
        "        if j == 0:\n",
        "            # å·¦ç«¯(-0.1ã®ä½ç½®)ã«ã€90åº¦å›è»¢ã•ã›ã¦åå‰ã‚’è¡¨ç¤º\n",
        "            ax.text(-0.1, 0.5, subject.upper(), transform=ax.transAxes,\n",
        "                    rotation=90, va='center', ha='right', fontsize=24, weight='bold')\n",
        "\n",
        "# --- ä¿å­˜ ---\n",
        "print(\"ğŸ’¾ é«˜ç”»è³ªã§ä¿å­˜ä¸­... (å°‘ã—æ™‚é–“ãŒã‹ã‹ã‚Šã¾ã™)\")\n",
        "plt.savefig(output_filename, dpi=150, bbox_inches='tight') # dpi=300ã ã¨é‡ã™ãã‚‹å ´åˆã¯150ã«\n",
        "plt.close()\n",
        "\n",
        "print(f\"âœ… å®Œäº†ã—ã¾ã—ãŸ: {output_filename}\")\n",
        "\n",
        "# ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰\n",
        "from google.colab import files\n",
        "files.download(output_filename)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "id": "gMICWRZKYWF8",
        "outputId": "09d5f64e-d8c7-40ca-9041-451021ae0584"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ–¼ï¸ ç”»åƒã‚’çµåˆä¸­...\n",
            "ğŸ’¾ é«˜ç”»è³ªã§ä¿å­˜ä¸­... (å°‘ã—æ™‚é–“ãŒã‹ã‹ã‚Šã¾ã™)\n",
            "âœ… å®Œäº†ã—ã¾ã—ãŸ: Combined_Matrix_5x10.png\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_e735be4e-faee-4eac-9337-8e3872802781\", \"Combined_Matrix_5x10.png\", 1242989)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# ã€ç”»åƒçµåˆã€‘5äººãšã¤2æšã«åˆ†å‰² (5è¡Œx5åˆ— x 2ãƒ•ã‚¡ã‚¤ãƒ«)\n",
        "#  è¦‹å‡ºã—(Phase/Name)ãªã—ç‰ˆ\n",
        "# =============================================================================\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "import os\n",
        "import math\n",
        "\n",
        "# --- è¨­å®š ---\n",
        "\n",
        "input_dir = \"Flow_Analysis_Results_Raw\"\n",
        "\n",
        "# å…¨ãƒªã‚¹ãƒˆ\n",
        "full_subject_list = [\n",
        "    'ishida', 'yamamoto', 'ohashi', 'miyake', 'mitsuhashi',\n",
        "    'ko', 'hayato', 'chika', 'nagomi', 'mio'\n",
        "]\n",
        "\n",
        "phase_list = ['rest', 'boredom', 'flow', 'flow_ultra', 'overload']\n",
        "\n",
        "# 5äººãšã¤ã«åˆ†å‰²\n",
        "groups = [\n",
        "    full_subject_list[:5],  # Part 1 (å‰åŠ5äºº)\n",
        "    full_subject_list[5:]   # Part 2 (å¾ŒåŠ5äºº)\n",
        "]\n",
        "\n",
        "output_filenames = [\"Combined_Part1_Top5.png\", \"Combined_Part2_Bottom5.png\"]\n",
        "\n",
        "# --- çµåˆå‡¦ç† ---\n",
        "\n",
        "print(\"ğŸ–¼ï¸ ç”»åƒçµåˆã‚’é–‹å§‹ã—ã¾ã™ (è¦‹å‡ºã—ãªã— / 5x5åˆ†å‰²)...\")\n",
        "\n",
        "for group_idx, current_subjects in enumerate(groups):\n",
        "\n",
        "    # 5è¡Œ x 5åˆ— ã®ã‚­ãƒ£ãƒ³ãƒã‚¹\n",
        "    # figsize=(å¹…, é«˜ã•) -> 20, 15 ãã‚‰ã„ãŒA4ã«è²¼ã‚Šã‚„ã™ã„æ¯”ç‡ã§ã™\n",
        "    fig, axes = plt.subplots(nrows=5, ncols=5, figsize=(20, 15))\n",
        "\n",
        "    # ã‚°ãƒ©ãƒ•é–“ã®éš™é–“èª¿æ•´ (0ã«ã™ã‚‹ã¨ãã£ã¤ãã¾ã™ã€‚é©å®œèª¿æ•´ã—ã¦ãã ã•ã„)\n",
        "    plt.subplots_adjust(wspace=0.05, hspace=0.1)\n",
        "\n",
        "    for i, subject in enumerate(current_subjects):\n",
        "        for j, phase in enumerate(phase_list):\n",
        "\n",
        "            ax = axes[i, j]\n",
        "\n",
        "            # ç”»åƒèª­ã¿è¾¼ã¿\n",
        "            filename = f\"{subject}_{phase}.png\"\n",
        "            filepath = os.path.join(input_dir, filename)\n",
        "\n",
        "            if os.path.exists(filepath):\n",
        "                img = mpimg.imread(filepath)\n",
        "                ax.imshow(img)\n",
        "            else:\n",
        "                # ç”»åƒãŒãªã„å ´åˆ\n",
        "                ax.text(0.5, 0.5, \"No Data\", ha='center', va='center', color='gray')\n",
        "\n",
        "            # æ ç·šãƒ»ç›®ç››ã‚Šãƒ»ãƒ©ãƒ™ãƒ«ã‚’å…¨ã¦æ¶ˆã™ (ç´”ç²‹ã«ç”»åƒã‚’è²¼ã‚‹ã ã‘)\n",
        "            ax.axis('off')\n",
        "\n",
        "    # ä¿å­˜\n",
        "    save_name = output_filenames[group_idx]\n",
        "    print(f\"ğŸ’¾ ä¿å­˜ä¸­: {save_name} ...\")\n",
        "\n",
        "    # dpi=300 ã§é«˜ç”»è³ªä¿å­˜ (è«–æ–‡ç”¨)\n",
        "    plt.savefig(save_name, dpi=300, bbox_inches='tight', pad_inches=0.1)\n",
        "    plt.close()\n",
        "\n",
        "print(\"\\nâœ… å…¨ã¦ã®å‡¦ç†ãŒå®Œäº†ã—ã¾ã—ãŸã€‚\")\n",
        "\n",
        "# ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰\n",
        "from google.colab import files\n",
        "for f in output_filenames:\n",
        "    if os.path.exists(f):\n",
        "        files.download(f)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "id": "I463SbL-ZgpS",
        "outputId": "aa60c58d-4bd8-4430-a6ac-84a02470ed29"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ–¼ï¸ ç”»åƒçµåˆã‚’é–‹å§‹ã—ã¾ã™ (è¦‹å‡ºã—ãªã— / 5x5åˆ†å‰²)...\n",
            "ğŸ’¾ ä¿å­˜ä¸­: Combined_Part1_Top5.png ...\n",
            "ğŸ’¾ ä¿å­˜ä¸­: Combined_Part2_Bottom5.png ...\n",
            "\n",
            "âœ… å…¨ã¦ã®å‡¦ç†ãŒå®Œäº†ã—ã¾ã—ãŸã€‚\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_1c6f3b61-ae50-4d99-9206-a5ec87659a4e\", \"Combined_Part1_Top5.png\", 1105499)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_385a2f15-0917-40c0-bb49-e633a15e592c\", \"Combined_Part2_Bottom5.png\", 1105660)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# ã€ç”»åƒçµåˆã€‘5äººÃ—5ãƒ•ã‚§ãƒ¼ã‚º (2åˆ†å‰²)\n",
        "#  â˜…æ”¹è‰¯ç‰ˆ: ä½™ç™½ã‚’æ¥µé™ã¾ã§å‰Šé™¤ã—ã€ã‚°ãƒ©ãƒ•ã‚’æœ€å¤§åŒ–ã™ã‚‹\n",
        "# =============================================================================\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "import os\n",
        "\n",
        "# --- è¨­å®š ---\n",
        "\n",
        "input_dir = \"Flow_Analysis_Results_Raw\"\n",
        "\n",
        "# å…¨ãƒªã‚¹ãƒˆ\n",
        "full_subject_list = [\n",
        "    'ishida', 'yamamoto', 'ohashi', 'miyake', 'mitsuhashi',\n",
        "    'ko', 'hayato', 'chika', 'nagomi', 'mio'\n",
        "]\n",
        "\n",
        "phase_list = ['rest', 'boredom', 'flow', 'flow_ultra', 'overload']\n",
        "\n",
        "# 5äººãšã¤ã«åˆ†å‰²\n",
        "groups = [\n",
        "    full_subject_list[:5],  # Part 1 (å‰åŠ5äºº)\n",
        "    full_subject_list[5:]   # Part 2 (å¾ŒåŠ5äºº)\n",
        "]\n",
        "\n",
        "output_filenames = [\"Combined_Part1_Top5_Max.png\", \"Combined_Part2_Bottom5_Max.png\"]\n",
        "\n",
        "# --- çµåˆå‡¦ç† ---\n",
        "\n",
        "print(\"ğŸ–¼ï¸ ç”»åƒçµåˆã‚’é–‹å§‹ã—ã¾ã™ (ä½™ç™½ã‚¼ãƒ­ãƒ»æœ€å¤§åŒ–ãƒ¢ãƒ¼ãƒ‰)...\")\n",
        "\n",
        "for group_idx, current_subjects in enumerate(groups):\n",
        "\n",
        "    # â˜…ãƒã‚¤ãƒ³ãƒˆ1: figsizeã®æ¯”ç‡ã‚’èª¿æ•´\n",
        "    # å…ƒã®ã‚°ãƒ©ãƒ•ãŒæ¨ªé•·ãªã®ã§ã€æ¨ªå¹…ã‚’åºƒãå–ã‚Šã¾ã™ (25ã‚¤ãƒ³ãƒ x 10ã‚¤ãƒ³ãƒ)\n",
        "    # ã“ã‚Œã«ã‚ˆã‚Šã€å€‹ã€…ã®ã‚°ãƒ©ãƒ•ãŒæŠ¼ã—ã¤ã¶ã•ã‚Œãšã«ãƒ”ã‚¿ãƒªã¨åã¾ã‚Šã¾ã™\n",
        "    fig, axes = plt.subplots(nrows=5, ncols=5, figsize=(25, 10),\n",
        "                             gridspec_kw={'wspace': 0, 'hspace': 0}) # â˜…ãƒã‚¤ãƒ³ãƒˆ2: ã‚°ãƒ©ãƒ•é–“ã®éš™é–“ã‚’0ã«ã™ã‚‹\n",
        "\n",
        "    # ã‚­ãƒ£ãƒ³ãƒã‚¹è‡ªä½“ã®ä½™ç™½ã‚‚å‰Šé™¤\n",
        "    plt.subplots_adjust(left=0, right=1, bottom=0, top=1)\n",
        "\n",
        "    for i, subject in enumerate(current_subjects):\n",
        "        for j, phase in enumerate(phase_list):\n",
        "\n",
        "            ax = axes[i, j]\n",
        "\n",
        "            # ç”»åƒèª­ã¿è¾¼ã¿\n",
        "            filename = f\"{subject}_{phase}.png\"\n",
        "            filepath = os.path.join(input_dir, filename)\n",
        "\n",
        "            if os.path.exists(filepath):\n",
        "                img = mpimg.imread(filepath)\n",
        "                # aspect='auto' ã«ã™ã‚‹ã¨ã€æ ã«åˆã‚ã›ã¦ç”»åƒã‚’å¤šå°‘ä¼¸ç¸®ã•ã›ã¦ã§ã‚‚åŸ‹ã‚ã¾ã™\n",
        "                # ç”»åƒã®ç¸¦æ¨ªæ¯”ã‚’å³å¯†ã«ç¶­æŒã—ãŸã„å ´åˆã¯ 'equal' ã«æˆ»ã—ã¦ãã ã•ã„\n",
        "                ax.imshow(img, aspect='auto')\n",
        "            else:\n",
        "                ax.text(0.5, 0.5, \"No Data\", ha='center', va='center', color='gray')\n",
        "\n",
        "            # è»¸ã‚’ã‚ªãƒ•\n",
        "            ax.axis('off')\n",
        "\n",
        "    # ä¿å­˜\n",
        "    save_name = output_filenames[group_idx]\n",
        "    print(f\"ğŸ’¾ ä¿å­˜ä¸­: {save_name} ...\")\n",
        "\n",
        "    # â˜…ãƒã‚¤ãƒ³ãƒˆ3: pad_inches=0 ã§å¤–å´ã®ä½™ç™½ã‚‚å®Œå…¨ã‚«ãƒƒãƒˆ\n",
        "    plt.savefig(save_name, dpi=300, bbox_inches='tight', pad_inches=0)\n",
        "    plt.close()\n",
        "\n",
        "print(\"\\nâœ… å®Œäº†ã—ã¾ã—ãŸã€‚\")\n",
        "\n",
        "# ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰\n",
        "from google.colab import files\n",
        "for f in output_filenames:\n",
        "    if os.path.exists(f):\n",
        "        files.download(f)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "id": "2oTuo83cZ8ne",
        "outputId": "75beaa92-5484-45d4-a7f1-7953242fac26"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ–¼ï¸ ç”»åƒçµåˆã‚’é–‹å§‹ã—ã¾ã™ (ä½™ç™½ã‚¼ãƒ­ãƒ»æœ€å¤§åŒ–ãƒ¢ãƒ¼ãƒ‰)...\n",
            "ğŸ’¾ ä¿å­˜ä¸­: Combined_Part1_Top5_Max.png ...\n",
            "ğŸ’¾ ä¿å­˜ä¸­: Combined_Part2_Bottom5_Max.png ...\n",
            "\n",
            "âœ… å®Œäº†ã—ã¾ã—ãŸã€‚\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_3cc275f9-243d-4e49-a0e2-823394bc41db\", \"Combined_Part1_Top5_Max.png\", 2276619)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_106b8621-e035-4bb6-a1bb-84e0f0e98282\", \"Combined_Part2_Bottom5_Max.png\", 2274859)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# ã€æ¥µå¤ªãƒ»é«˜è¦–èªæ€§ç‰ˆã€‘EEG & HRV è§£æã‚³ãƒ¼ãƒ‰\n",
        "# =============================================================================\n",
        "# å¤‰æ›´ç‚¹:\n",
        "# 1. linewidth (ç·šã®å¤ªã•) ã‚’å¤§å¹…ã«ã‚¢ãƒƒãƒ—\n",
        "# 2. fontsize (æ–‡å­—ã‚µã‚¤ã‚º) ã‚’ã‚¢ãƒƒãƒ—ã—ã€fontweight='bold' (å¤ªå­—) ã«è¨­å®š\n",
        "# =============================================================================\n",
        "\n",
        "import sys\n",
        "import os\n",
        "import glob\n",
        "import shutil\n",
        "import warnings\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy import signal\n",
        "from scipy.interpolate import interp1d\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from google.colab import drive\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# --- è¨­å®š ---\n",
        "if not os.path.exists('/content/drive'):\n",
        "    drive.mount('/content/drive')\n",
        "\n",
        "base_search_path = \"/content/drive/MyDrive/csv\"\n",
        "\n",
        "subject_list = [\n",
        "    'ishida', 'yamamoto', 'ohashi', 'miyake', 'mitsuhashi',\n",
        "    'ko', 'hayato', 'chika', 'nagomi', 'mio'\n",
        "]\n",
        "\n",
        "phase_list = ['rest', 'boredom', 'flow', 'flow_ultra', 'overload']\n",
        "\n",
        "output_dir = \"Flow_Analysis_Results_Bold\" # å‡ºåŠ›å…ˆã‚’å¤‰æ›´\n",
        "if os.path.exists(output_dir):\n",
        "    shutil.rmtree(output_dir)\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "# --- å…±é€šé–¢æ•° (å¤‰æ›´ãªã—) ---\n",
        "def find_file_fuzzy(base_path, subject, phase, keywords):\n",
        "    search_pattern = os.path.join(base_path, \"**\", f\"*{subject}*{phase}*.csv\")\n",
        "    candidates = glob.glob(search_pattern, recursive=True)\n",
        "    for path in candidates:\n",
        "        if any(k.lower() in os.path.basename(path).lower() for k in keywords):\n",
        "            return path\n",
        "    return None\n",
        "\n",
        "def get_column_data(df, target_names):\n",
        "    df.columns = [str(c).strip() for c in df.columns]\n",
        "    for col in df.columns:\n",
        "        for target in target_names:\n",
        "            if col.lower() == target.lower():\n",
        "                return df[col], col\n",
        "    return None, None\n",
        "\n",
        "def smart_extract_rr_data(df):\n",
        "    series, name = get_column_data(df, ['rr_interval', 'rr', 'RR', 'interval', 'R-R', 'bpm'])\n",
        "    if series is not None: return series, name\n",
        "    for col in df.columns:\n",
        "        try:\n",
        "            val = pd.to_numeric(df[col], errors='coerce').dropna()\n",
        "            if len(val) < 10: continue\n",
        "            mean_val = val.mean()\n",
        "            if 300 < mean_val < 1500: return val, f\"{col} (Auto ms)\"\n",
        "            if 0.3 < mean_val < 1.5: return val * 1000, f\"{col} (Auto sec)\"\n",
        "        except: continue\n",
        "    return None, None\n",
        "\n",
        "def calculate_lf_high_res(rr_data_ms):\n",
        "    try:\n",
        "        rr_data = rr_data_ms[(rr_data_ms > 300) & (rr_data_ms < 2000)]\n",
        "        if len(rr_data) < 10: return None\n",
        "        t_cum = np.cumsum(rr_data) / 1000.0\n",
        "        t_cum = t_cum - t_cum.iloc[0]\n",
        "        fs_interp = 4.0\n",
        "        t_interp = np.arange(0, t_cum.iloc[-1], 1/fs_interp)\n",
        "        f_int = interp1d(t_cum, rr_data, kind='linear', fill_value=\"extrapolate\")\n",
        "        rr_interp = f_int(t_interp)\n",
        "\n",
        "        window_sec = 60\n",
        "        step_sec = 1\n",
        "        nperseg = int(window_sec * fs_interp)\n",
        "        step_samples = int(step_sec * fs_interp)\n",
        "        if len(rr_interp) < nperseg: return None\n",
        "\n",
        "        lf_vals = []\n",
        "        t_vals = []\n",
        "        for i in range(0, len(rr_interp) - nperseg, step_samples):\n",
        "            segment = rr_interp[i : i + nperseg]\n",
        "            freqs, psd = signal.welch(segment, fs=fs_interp, nperseg=nperseg)\n",
        "            lf_pow = np.trapz(psd[(freqs >= 0.04) & (freqs <= 0.15)], freqs[(freqs >= 0.04) & (freqs <= 0.15)])\n",
        "            lf_vals.append(lf_pow)\n",
        "            t_vals.append(t_interp[i + nperseg//2])\n",
        "\n",
        "        x_target = np.linspace(0, t_cum.iloc[-1], len(rr_data_ms))\n",
        "        if len(t_vals) > 1:\n",
        "            f_res = interp1d(t_vals, lf_vals, kind='linear', fill_value=\"extrapolate\")\n",
        "            return pd.Series(f_res(x_target), index=rr_data_ms.index)\n",
        "        return None\n",
        "    except: return None\n",
        "\n",
        "def min_max_norm(series):\n",
        "    if len(series) == 0: return series\n",
        "    scaler = MinMaxScaler()\n",
        "    return scaler.fit_transform(series.values.reshape(-1, 1)).flatten()\n",
        "\n",
        "# --- ãƒ¡ã‚¤ãƒ³å‡¦ç† ---\n",
        "print(f\"ğŸš€ è§£æé–‹å§‹ (æ¥µå¤ªãƒ©ã‚¤ãƒ³ãƒ»å¤ªæ–‡å­—ãƒ¢ãƒ¼ãƒ‰)\\n\")\n",
        "\n",
        "# â˜…å…¨ä½“ã®æ–‡å­—ã‚µã‚¤ã‚ºã‚’ä¸€æ‹¬è¨­å®š\n",
        "plt.rcParams.update({'font.size': 14, 'font.weight': 'bold', 'axes.labelweight': 'bold'})\n",
        "\n",
        "for subject in subject_list:\n",
        "    for phase in phase_list:\n",
        "        print(f\"[{subject} - {phase}]\", end=\" \")\n",
        "\n",
        "        path_eeg = find_file_fuzzy(base_search_path, subject, phase, ['PLI', 'pli'])\n",
        "        path_rr = find_file_fuzzy(base_search_path, subject, phase, ['rr_interval', 'RR', 'rr', 'interval'])\n",
        "\n",
        "        if not path_eeg or not path_rr:\n",
        "            print(\"-> Skip\")\n",
        "            continue\n",
        "\n",
        "        try:\n",
        "            df_eeg = pd.read_csv(path_eeg)\n",
        "            df_rr = pd.read_csv(path_rr)\n",
        "            eeg_data, _ = get_column_data(df_eeg, ['FCz', 'fcz'])\n",
        "            rr_vals, _ = smart_extract_rr_data(df_rr)\n",
        "\n",
        "            if eeg_data is None or rr_vals is None: continue\n",
        "\n",
        "            eeg_data = eeg_data.fillna(method='ffill').fillna(method='bfill')\n",
        "            lf_data = calculate_lf_high_res(rr_vals)\n",
        "            if lf_data is None: continue\n",
        "\n",
        "            min_len = min(len(eeg_data), len(lf_data))\n",
        "            eeg_vis = eeg_data.iloc[:min_len]\n",
        "            lf_vis = lf_data.iloc[:min_len]\n",
        "\n",
        "            fig, ax1 = plt.subplots(figsize=(12, 5))\n",
        "\n",
        "            # â˜…ç·šã®å¤ªã•ã‚’å¤‰æ›´ (linewidth)\n",
        "            # EEG: é’è‰², å¤ªã• 2.0\n",
        "            ax1.plot(min_max_norm(eeg_vis), color='blue', label='EEG', alpha=0.7, linewidth=2.0)\n",
        "            ax1.set_ylabel('EEG Power', color='blue', fontsize=16, fontweight='bold')\n",
        "            ax1.tick_params(axis='y', labelcolor='blue', labelsize=12)\n",
        "\n",
        "            # LF: ã‚ªãƒ¬ãƒ³ã‚¸, å¤ªã• 3.0 (ä¸»å½¹ãªã®ã§ã‚ˆã‚Šå¤ªã)\n",
        "            ax2 = ax1.twinx()\n",
        "            ax2.plot(min_max_norm(lf_vis), color='orange', label='HRV (LF)', alpha=1.0, linewidth=3.5)\n",
        "            ax2.set_ylabel('LF Power', color='orange', fontsize=16, fontweight='bold')\n",
        "            ax2.tick_params(axis='y', labelcolor='orange', labelsize=12)\n",
        "\n",
        "            # ã‚¿ã‚¤ãƒˆãƒ«ã‚’å‰Šé™¤ï¼ˆã‚ã¨ã§çµåˆã™ã‚‹ãªã‚‰é‚ªé­”ã«ãªã‚‹ãŸã‚ã€‚å¿…è¦ãªã‚‰ã‚³ãƒ¡ãƒ³ãƒˆã‚¢ã‚¦ãƒˆè§£é™¤ï¼‰\n",
        "            # plt.title(f\"{subject} - {phase}\", fontsize=18, fontweight='bold')\n",
        "\n",
        "            # å‡¡ä¾‹ (Legend) ã‚’å¤§ãã\n",
        "            lines1, labels1 = ax1.get_legend_handles_labels()\n",
        "            lines2, labels2 = ax2.get_legend_handles_labels()\n",
        "            ax1.legend(lines1 + lines2, labels1 + labels2, loc='upper left', fontsize=14, frameon=True)\n",
        "\n",
        "            plt.savefig(os.path.join(output_dir, f\"{subject}_{phase}.png\"))\n",
        "            plt.close()\n",
        "            print(\"âœ… OK\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"-> Error: {e}\")\n",
        "\n",
        "print(\"\\nğŸ‰ ç”»åƒç”Ÿæˆå®Œäº†\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sf4y3J4HaZmV",
        "outputId": "022e43df-3fb7-488d-e752-ee44db6fc2f5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸš€ è§£æé–‹å§‹ (æ¥µå¤ªãƒ©ã‚¤ãƒ³ãƒ»å¤ªæ–‡å­—ãƒ¢ãƒ¼ãƒ‰)\n",
            "\n",
            "[ishida - rest] âœ… OK\n",
            "[ishida - boredom] âœ… OK\n",
            "[ishida - flow] âœ… OK\n",
            "[ishida - flow_ultra] âœ… OK\n",
            "[ishida - overload] âœ… OK\n",
            "[yamamoto - rest] âœ… OK\n",
            "[yamamoto - boredom] âœ… OK\n",
            "[yamamoto - flow] âœ… OK\n",
            "[yamamoto - flow_ultra] âœ… OK\n",
            "[yamamoto - overload] âœ… OK\n",
            "[ohashi - rest] âœ… OK\n",
            "[ohashi - boredom] âœ… OK\n",
            "[ohashi - flow] âœ… OK\n",
            "[ohashi - flow_ultra] âœ… OK\n",
            "[ohashi - overload] âœ… OK\n",
            "[miyake - rest] âœ… OK\n",
            "[miyake - boredom] âœ… OK\n",
            "[miyake - flow] âœ… OK\n",
            "[miyake - flow_ultra] âœ… OK\n",
            "[miyake - overload] âœ… OK\n",
            "[mitsuhashi - rest] âœ… OK\n",
            "[mitsuhashi - boredom] âœ… OK\n",
            "[mitsuhashi - flow] âœ… OK\n",
            "[mitsuhashi - flow_ultra] âœ… OK\n",
            "[mitsuhashi - overload] âœ… OK\n",
            "[ko - rest] âœ… OK\n",
            "[ko - boredom] âœ… OK\n",
            "[ko - flow] âœ… OK\n",
            "[ko - flow_ultra] âœ… OK\n",
            "[ko - overload] âœ… OK\n",
            "[hayato - rest] âœ… OK\n",
            "[hayato - boredom] âœ… OK\n",
            "[hayato - flow] âœ… OK\n",
            "[hayato - flow_ultra] âœ… OK\n",
            "[hayato - overload] âœ… OK\n",
            "[chika - rest] âœ… OK\n",
            "[chika - boredom] âœ… OK\n",
            "[chika - flow] âœ… OK\n",
            "[chika - flow_ultra] âœ… OK\n",
            "[chika - overload] âœ… OK\n",
            "[nagomi - rest] âœ… OK\n",
            "[nagomi - boredom] âœ… OK\n",
            "[nagomi - flow] âœ… OK\n",
            "[nagomi - flow_ultra] âœ… OK\n",
            "[nagomi - overload] âœ… OK\n",
            "[mio - rest] âœ… OK\n",
            "[mio - boredom] âœ… OK\n",
            "[mio - flow] âœ… OK\n",
            "[mio - flow_ultra] âœ… OK\n",
            "[mio - overload] âœ… OK\n",
            "\n",
            "ğŸ‰ ç”»åƒç”Ÿæˆå®Œäº†\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# ã€é¡Œåå¾©æ´»ãƒ»æ¥µå¤ªç‰ˆã€‘EEG & HRV è§£æã‚³ãƒ¼ãƒ‰\n",
        "# =============================================================================\n",
        "\n",
        "import sys\n",
        "import os\n",
        "import glob\n",
        "import shutil\n",
        "import warnings\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy import signal\n",
        "from scipy.interpolate import interp1d\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from google.colab import drive\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# --- è¨­å®š ---\n",
        "if not os.path.exists('/content/drive'):\n",
        "    drive.mount('/content/drive')\n",
        "\n",
        "base_search_path = \"/content/drive/MyDrive/csv\"\n",
        "\n",
        "subject_list = [\n",
        "    'ishida', 'yamamoto', 'ohashi', 'miyake', 'mitsuhashi',\n",
        "    'ko', 'hayato', 'chika', 'nagomi', 'mio'\n",
        "]\n",
        "\n",
        "phase_list = ['rest', 'boredom', 'flow', 'flow_ultra', 'overload']\n",
        "\n",
        "# å‡ºåŠ›å…ˆã‚’æ–°ã—ã„åå‰ã«\n",
        "output_dir = \"Flow_Results_Bold_Title\"\n",
        "if os.path.exists(output_dir):\n",
        "    shutil.rmtree(output_dir)\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "# --- å…±é€šé–¢æ•° ---\n",
        "def find_file_fuzzy(base_path, subject, phase, keywords):\n",
        "    search_pattern = os.path.join(base_path, \"**\", f\"*{subject}*{phase}*.csv\")\n",
        "    candidates = glob.glob(search_pattern, recursive=True)\n",
        "    for path in candidates:\n",
        "        if any(k.lower() in os.path.basename(path).lower() for k in keywords):\n",
        "            return path\n",
        "    return None\n",
        "\n",
        "def get_column_data(df, target_names):\n",
        "    df.columns = [str(c).strip() for c in df.columns]\n",
        "    for col in df.columns:\n",
        "        for target in target_names:\n",
        "            if col.lower() == target.lower():\n",
        "                return df[col], col\n",
        "    return None, None\n",
        "\n",
        "def smart_extract_rr_data(df):\n",
        "    series, name = get_column_data(df, ['rr_interval', 'rr', 'RR', 'interval', 'R-R', 'bpm'])\n",
        "    if series is not None: return series, name\n",
        "    for col in df.columns:\n",
        "        try:\n",
        "            val = pd.to_numeric(df[col], errors='coerce').dropna()\n",
        "            if len(val) < 10: continue\n",
        "            mean_val = val.mean()\n",
        "            if 300 < mean_val < 1500: return val, f\"{col} (Auto ms)\"\n",
        "            if 0.3 < mean_val < 1.5: return val * 1000, f\"{col} (Auto sec)\"\n",
        "        except: continue\n",
        "    return None, None\n",
        "\n",
        "def calculate_lf_high_res(rr_data_ms):\n",
        "    try:\n",
        "        rr_data = rr_data_ms[(rr_data_ms > 300) & (rr_data_ms < 2000)]\n",
        "        if len(rr_data) < 10: return None\n",
        "        t_cum = np.cumsum(rr_data) / 1000.0\n",
        "        t_cum = t_cum - t_cum.iloc[0]\n",
        "        fs_interp = 4.0\n",
        "        t_interp = np.arange(0, t_cum.iloc[-1], 1/fs_interp)\n",
        "        f_int = interp1d(t_cum, rr_data, kind='linear', fill_value=\"extrapolate\")\n",
        "        rr_interp = f_int(t_interp)\n",
        "\n",
        "        window_sec = 60\n",
        "        step_sec = 1\n",
        "        nperseg = int(window_sec * fs_interp)\n",
        "        step_samples = int(step_sec * fs_interp)\n",
        "        if len(rr_interp) < nperseg: return None\n",
        "\n",
        "        lf_vals = []\n",
        "        t_vals = []\n",
        "        for i in range(0, len(rr_interp) - nperseg, step_samples):\n",
        "            segment = rr_interp[i : i + nperseg]\n",
        "            freqs, psd = signal.welch(segment, fs=fs_interp, nperseg=nperseg)\n",
        "            lf_pow = np.trapz(psd[(freqs >= 0.04) & (freqs <= 0.15)], freqs[(freqs >= 0.04) & (freqs <= 0.15)])\n",
        "            lf_vals.append(lf_pow)\n",
        "            t_vals.append(t_interp[i + nperseg//2])\n",
        "\n",
        "        x_target = np.linspace(0, t_cum.iloc[-1], len(rr_data_ms))\n",
        "        if len(t_vals) > 1:\n",
        "            f_res = interp1d(t_vals, lf_vals, kind='linear', fill_value=\"extrapolate\")\n",
        "            return pd.Series(f_res(x_target), index=rr_data_ms.index)\n",
        "        return None\n",
        "    except: return None\n",
        "\n",
        "def min_max_norm(series):\n",
        "    if len(series) == 0: return series\n",
        "    scaler = MinMaxScaler()\n",
        "    return scaler.fit_transform(series.values.reshape(-1, 1)).flatten()\n",
        "\n",
        "# --- ãƒ¡ã‚¤ãƒ³å‡¦ç† ---\n",
        "print(f\"ğŸš€ è§£æé–‹å§‹ (é¡Œåä»˜ããƒ»æ¥µå¤ªãƒ¢ãƒ¼ãƒ‰)\\n\")\n",
        "\n",
        "# ãƒ•ã‚©ãƒ³ãƒˆè¨­å®š\n",
        "plt.rcParams.update({'font.size': 14, 'font.weight': 'bold', 'axes.labelweight': 'bold'})\n",
        "\n",
        "for subject in subject_list:\n",
        "    for phase in phase_list:\n",
        "        print(f\"[{subject} - {phase}]\", end=\" \")\n",
        "\n",
        "        path_eeg = find_file_fuzzy(base_search_path, subject, phase, ['PLI', 'pli'])\n",
        "        path_rr = find_file_fuzzy(base_search_path, subject, phase, ['rr_interval', 'RR', 'rr', 'interval'])\n",
        "\n",
        "        if not path_eeg or not path_rr:\n",
        "            print(\"-> Skip\")\n",
        "            continue\n",
        "\n",
        "        try:\n",
        "            df_eeg = pd.read_csv(path_eeg)\n",
        "            df_rr = pd.read_csv(path_rr)\n",
        "            eeg_data, _ = get_column_data(df_eeg, ['FCz', 'fcz'])\n",
        "            rr_vals, _ = smart_extract_rr_data(df_rr)\n",
        "\n",
        "            if eeg_data is None or rr_vals is None: continue\n",
        "\n",
        "            eeg_data = eeg_data.fillna(method='ffill').fillna(method='bfill')\n",
        "            lf_data = calculate_lf_high_res(rr_vals)\n",
        "            if lf_data is None: continue\n",
        "\n",
        "            min_len = min(len(eeg_data), len(lf_data))\n",
        "            eeg_vis = eeg_data.iloc[:min_len]\n",
        "            lf_vis = lf_data.iloc[:min_len]\n",
        "\n",
        "            fig, ax1 = plt.subplots(figsize=(12, 5))\n",
        "\n",
        "            # ç·šã‚’å¤ªã\n",
        "            ax1.plot(min_max_norm(eeg_vis), color='blue', label='EEG', alpha=0.7, linewidth=2.0)\n",
        "            ax1.set_ylabel('EEG Power', color='blue', fontsize=16, fontweight='bold')\n",
        "            ax1.tick_params(axis='y', labelcolor='blue', labelsize=12)\n",
        "\n",
        "            # LFã‚’ã•ã‚‰ã«å¤ªã\n",
        "            ax2 = ax1.twinx()\n",
        "            ax2.plot(min_max_norm(lf_vis), color='orange', label='HRV (LF)', alpha=1.0, linewidth=3.5)\n",
        "            ax2.set_ylabel('LF Power', color='orange', fontsize=16, fontweight='bold')\n",
        "            ax2.tick_params(axis='y', labelcolor='orange', labelsize=12)\n",
        "\n",
        "            # â˜…ã‚¿ã‚¤ãƒˆãƒ«ã‚’å¾©æ´» (å¤§ããå¤ªã)\n",
        "            title_str = f\"{subject.upper()} - {phase.upper()}\"\n",
        "            plt.title(title_str, fontsize=20, fontweight='bold', pad=15)\n",
        "\n",
        "            lines1, labels1 = ax1.get_legend_handles_labels()\n",
        "            lines2, labels2 = ax2.get_legend_handles_labels()\n",
        "            ax1.legend(lines1 + lines2, labels1 + labels2, loc='upper left', fontsize=14, frameon=True)\n",
        "\n",
        "            # ã‚¿ã‚¤ãƒˆãƒ«ãŒåˆ‡ã‚Œãªã„ã‚ˆã†ã« bbox_inches='tight' ã§ä¿å­˜\n",
        "            plt.savefig(os.path.join(output_dir, f\"{subject}_{phase}.png\"), bbox_inches='tight')\n",
        "            plt.close()\n",
        "            print(\"âœ… OK\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"-> Error: {e}\")\n",
        "\n",
        "print(\"\\nğŸ‰ ç”»åƒç”Ÿæˆå®Œäº†\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z_v5ObB8dKAV",
        "outputId": "58e38500-262f-4ad5-941a-a11d3460336a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸš€ è§£æé–‹å§‹ (é¡Œåä»˜ããƒ»æ¥µå¤ªãƒ¢ãƒ¼ãƒ‰)\n",
            "\n",
            "[ishida - rest] âœ… OK\n",
            "[ishida - boredom] âœ… OK\n",
            "[ishida - flow] âœ… OK\n",
            "[ishida - flow_ultra] âœ… OK\n",
            "[ishida - overload] âœ… OK\n",
            "[yamamoto - rest] âœ… OK\n",
            "[yamamoto - boredom] âœ… OK\n",
            "[yamamoto - flow] âœ… OK\n",
            "[yamamoto - flow_ultra] âœ… OK\n",
            "[yamamoto - overload] âœ… OK\n",
            "[ohashi - rest] âœ… OK\n",
            "[ohashi - boredom] âœ… OK\n",
            "[ohashi - flow] âœ… OK\n",
            "[ohashi - flow_ultra] âœ… OK\n",
            "[ohashi - overload] âœ… OK\n",
            "[miyake - rest] âœ… OK\n",
            "[miyake - boredom] âœ… OK\n",
            "[miyake - flow] âœ… OK\n",
            "[miyake - flow_ultra] âœ… OK\n",
            "[miyake - overload] âœ… OK\n",
            "[mitsuhashi - rest] âœ… OK\n",
            "[mitsuhashi - boredom] âœ… OK\n",
            "[mitsuhashi - flow] âœ… OK\n",
            "[mitsuhashi - flow_ultra] âœ… OK\n",
            "[mitsuhashi - overload] âœ… OK\n",
            "[ko - rest] âœ… OK\n",
            "[ko - boredom] âœ… OK\n",
            "[ko - flow] âœ… OK\n",
            "[ko - flow_ultra] âœ… OK\n",
            "[ko - overload] âœ… OK\n",
            "[hayato - rest] âœ… OK\n",
            "[hayato - boredom] âœ… OK\n",
            "[hayato - flow] âœ… OK\n",
            "[hayato - flow_ultra] âœ… OK\n",
            "[hayato - overload] âœ… OK\n",
            "[chika - rest] âœ… OK\n",
            "[chika - boredom] âœ… OK\n",
            "[chika - flow] âœ… OK\n",
            "[chika - flow_ultra] âœ… OK\n",
            "[chika - overload] âœ… OK\n",
            "[nagomi - rest] âœ… OK\n",
            "[nagomi - boredom] âœ… OK\n",
            "[nagomi - flow] âœ… OK\n",
            "[nagomi - flow_ultra] âœ… OK\n",
            "[nagomi - overload] âœ… OK\n",
            "[mio - rest] âœ… OK\n",
            "[mio - boredom] âœ… OK\n",
            "[mio - flow] âœ… OK\n",
            "[mio - flow_ultra] âœ… OK\n",
            "[mio - overload] âœ… OK\n",
            "\n",
            "ğŸ‰ ç”»åƒç”Ÿæˆå®Œäº†\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# ã€ç”»åƒçµåˆã€‘é¡Œåä»˜ãç”»åƒã®çµåˆ (æœ€å¤§åŒ–ãƒ»ä½™ç™½å¾®èª¿æ•´)\n",
        "# =============================================================================\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "import os\n",
        "\n",
        "# â˜… é¡Œåä»˜ãã®ãƒ•ã‚©ãƒ«ãƒ€ã‚’æŒ‡å®š\n",
        "input_dir = \"Flow_Results_Bold_Title\"\n",
        "\n",
        "full_subject_list = [\n",
        "    'ishida', 'yamamoto', 'ohashi', 'miyake', 'mitsuhashi',\n",
        "    'ko', 'hayato', 'chika', 'nagomi', 'mio'\n",
        "]\n",
        "phase_list = ['rest', 'boredom', 'flow', 'flow_ultra', 'overload']\n",
        "\n",
        "groups = [full_subject_list[:5], full_subject_list[5:]]\n",
        "output_filenames = [\"Combined_Titled_Part1.png\", \"Combined_Titled_Part2.png\"]\n",
        "\n",
        "print(\"ğŸ–¼ï¸ é¡Œåä»˜ãç”»åƒã‚’çµåˆä¸­...\")\n",
        "\n",
        "for group_idx, current_subjects in enumerate(groups):\n",
        "    # figsize=(25, 12) å°‘ã—ç¸¦ã‚’é•·ãã—ã¦ã€ã‚¿ã‚¤ãƒˆãƒ«ãŒå…¥ã‚‹ä½™è£•ã‚’æŒãŸã›ã¾ã™\n",
        "    fig, axes = plt.subplots(nrows=5, ncols=5, figsize=(25, 12),\n",
        "                             gridspec_kw={'wspace': 0, 'hspace': 0.1})\n",
        "                             # hspace=0.1: é¡ŒåãŒä¸Šã®ã‚°ãƒ©ãƒ•ã¨è¢«ã‚‰ãªã„ã‚ˆã†ã«ã‚ãšã‹ã«éš™é–“ã‚’ç©ºã‘ã¾ã™\n",
        "\n",
        "    # å¤–æ ã®ä½™ç™½å‰Šé™¤\n",
        "    plt.subplots_adjust(left=0, right=1, bottom=0, top=1)\n",
        "\n",
        "    for i, subject in enumerate(current_subjects):\n",
        "        for j, phase in enumerate(phase_list):\n",
        "            ax = axes[i, j]\n",
        "            filename = f\"{subject}_{phase}.png\"\n",
        "            filepath = os.path.join(input_dir, filename)\n",
        "\n",
        "            if os.path.exists(filepath):\n",
        "                img = mpimg.imread(filepath)\n",
        "                ax.imshow(img, aspect='auto')\n",
        "            else:\n",
        "                ax.text(0.5, 0.5, \"No Data\", ha='center', va='center', color='gray')\n",
        "            ax.axis('off')\n",
        "\n",
        "    save_name = output_filenames[group_idx]\n",
        "    # pad_inches=0.1 ã‚ãšã‹ã«å¤–å´ã«ä½™ç™½ã‚’æ®‹ã—ã¦ã‚¿ã‚¤ãƒˆãƒ«åˆ‡ã‚Œé˜²æ­¢\n",
        "    plt.savefig(save_name, dpi=300, bbox_inches='tight', pad_inches=0.1)\n",
        "    plt.close()\n",
        "    print(f\"ğŸ’¾ ä¿å­˜å®Œäº†: {save_name}\")\n",
        "\n",
        "# ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰\n",
        "from google.colab import files\n",
        "for f in output_filenames:\n",
        "    if os.path.exists(f):\n",
        "        files.download(f)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "id": "cHbO__vSdVaZ",
        "outputId": "58fd48e6-5cbf-4855-ec25-19cc3bb22db8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ–¼ï¸ é¡Œåä»˜ãç”»åƒã‚’çµåˆä¸­...\n",
            "ğŸ’¾ ä¿å­˜å®Œäº†: Combined_Titled_Part1.png\n",
            "ğŸ’¾ ä¿å­˜å®Œäº†: Combined_Titled_Part2.png\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_7cbb8938-74cf-425d-bebc-83475ac99069\", \"Combined_Titled_Part1.png\", 3569709)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_8a0d46fc-0492-4743-87c1-20e86a51f739\", \"Combined_Titled_Part2.png\", 3532219)"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}
